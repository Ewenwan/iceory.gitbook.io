
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Softmax Loss with Symbolic Differentiation Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-etoc/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="pytorch_autograd.html" />
    
    
    <link rel="prev" href="gitbook.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../Network Quantization/Introduction.html">
            
                <a href="../Network Quantization/Introduction.html">
            
                    
                    Network Quantization
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../Network Quantization/Incremental Network Quantization.html">
            
                <a href="../Network Quantization/Incremental Network Quantization.html">
            
                    
                    Incremental Network Quantization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../Network Quantization/Dynamic Network Surgery.html">
            
                <a href="../Network Quantization/Dynamic Network Surgery.html">
            
                    
                    Dynamic Network Surgery
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../Network Quantization/Binary Weight Networks.html">
            
                <a href="../Network Quantization/Binary Weight Networks.html">
            
                    
                    Binary Weight Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../Network Quantization/Ternary Weight Networks.html">
            
                <a href="../Network Quantization/Ternary Weight Networks.html">
            
                    
                    Ternary Weight Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../Network Quantization/Trained Ternary Quantization.html">
            
                <a href="../Network Quantization/Trained Ternary Quantization.html">
            
                    
                    Trained Ternary Quantization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../Network Quantization/Binarized Neural Networks.html">
            
                <a href="../Network Quantization/Binarized Neural Networks.html">
            
                    
                    Binarized Neural Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../Network Quantization/Deep Compression.html">
            
                <a href="../Network Quantization/Deep Compression.html">
            
                    
                    Deep Compression
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    Notes
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="gitbook.html">
            
                <a href="gitbook.html">
            
                    
                    Install and Use Gitbook on Windows
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.2" data-path="softmaxloss.html">
            
                <a href="softmaxloss.html">
            
                    
                    Softmax Loss with Symbolic Differentiation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="pytorch_autograd.html">
            
                <a href="pytorch_autograd.html">
            
                    
                    PyTorch and Automatic Differentiation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="caffe2.html">
            
                <a href="caffe2.html">
            
                    
                    Install Caffe2 on Windows
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="markdown.html">
            
                <a href="markdown.html">
            
                    
                    Markdown Tutorial
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="vscode.html">
            
                <a href="vscode.html">
            
                    
                    VSCode
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.7" data-path="auxnet.html">
            
                <a href="auxnet.html">
            
                    
                    Re-implementation of AuxNet
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.8" data-path="git.html">
            
                <a href="git.html">
            
                    
                    Git
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.9" data-path="git.html">
            
                <a href="git.html">
            
                    
                    Power Point
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Softmax Loss with Symbolic Differentiation</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="manual-gradient-vs-auto-gradient-pytorch">Manual Gradient V.S. Auto Gradient (PyTorch)</h1>
<!-- toc --><div id="toc" class="toc">

<ul>
<li><a href="#reproduction-of-softmax-loss-with-cross-entropy">Reproduction of Softmax Loss with Cross Entropy</a><ul>
<li><a href="#softmax-function">softmax function</a></li>
<li><a href="#cross-entropy-loss">Cross entropy loss</a></li>
<li><a href="#coding-in-pytorch">Coding in PyTorch</a><ul>
<li><a href="#using-basic-function-of-pytorch">Using basic function of PyTorch</a></li>
<li><a href="#testing-code">testing code:</a></li>
</ul>
</li>
<li><a href="#reference">Reference:</a></li>
</ul>
</li>
</ul>

</div><!-- tocstop -->
<hr>
<p>github: <a href="https://github.com/ICEORY/softmax_loss_gradient.git" target="_blank">https://github.com/ICEORY/softmax_loss_gradient.git</a></p>
<h2 id="reproduction-of-softmax-loss-with-cross-entropy">Reproduction of Softmax Loss with Cross Entropy</h2>
<h3 id="softmax-function">softmax function</h3>
<p>the softmax function is defined by</p>
<p><script type="math/tex; ">
y_i = \frac{e^{x_i}}{\sum e^{x_k}}, for~i=1,..., C
</script>
where $x$ is the input with $C$ channels, $y$ is the respected output.</p>
<p>the gradient of softmax $\frac{\partial y_i}{\partial x_j}$ is computed by:</p>
<p><script type="math/tex; ">
{\rm if}~i=j,~\frac{\partial y_i}{\partial x_j}=\frac{\partial y_i}{\partial x_i} = \frac{e^{x_j}\cdot \sum e^{x_k}-e^{x_i}\cdot e^{x_i}}{(\sum e^{x_k})^2} = \frac{e^{x_i}}{\sum e^{x_k}}\frac{\sum e^{x_k}-e^{x_i}}{\sum e^{x_k}} = y_i \cdot (1-y_i)
</script></p>
<p><script type="math/tex; ">
{\rm if}~i\ne j,~\frac{\partial y_i}{\partial x_j}=\frac{\partial \frac{e^{x_j}}{\sum e^k}}{\partial x_j} = \frac{0\cdot \sum e^{x_k}-e^{x_i}\cdot e^{x_j}}{(\sum e^{x_k})^2} = -\frac{e^{x_i}}{\sum e^{x_k}} \frac{e^{x_j}}{\sum e^{x_k}} = -y_i \cdot y_j
</script></p>
<h3 id="cross-entropy-loss">Cross entropy loss</h3>
<p>the cross entropy loss function is </p>
<p><script type="math/tex; ">
\mathcal L = -\sum_{c=1}^C t_c\cdot log(y_c),
</script>
where $t$ is the one-hot label.</p>
<p>For a batch of samples, the cross-entropy loss can be re-written to</p>
<p><script type="math/tex; ">
\mathcal L = -\sum_{n=1}^N \sum_{c=1}^C t_{nc} \cdot log(y_{nc})
</script></p>
<p>the gradient of cross entropy loss is computed by</p>
<p><script type="math/tex; ">
\frac{\partial \mathcal L}{\partial x_i} = -\sum_{c=1}^C \frac{\partial t_c log (y_c)}{\partial x_i} = -\sum_{c=1}^C t_c \frac{\partial log(y_c)}{\partial x_i} = -\sum_{c=1}^C t_c\frac{1}{y_c}\frac{\partial y_c}{\partial x_i} = -\frac{t_i}{y_i}\frac{\partial {y_i}}{\partial x_i}-\sum_{i\ne j}^C \frac{t_j}{y_j}\frac{\partial y_j}{\partial x_i} = -\frac{t_i}{y_i}y_i(1-y_i) - \sum_{i\ne j}^C \frac{\partial t_j}{y_j}(-y_i y_j) = -t_i+t_i y_i + \sum_{i\ne j}^C t_j y_i = -t_i +y_i\sum_i^C t_i = y_i-t_i
</script></p>
<h3 id="coding-in-pytorch">Coding in PyTorch</h3>
<h4 id="using-basic-function-of-pytorch">Using basic function of PyTorch</h4>
<h5 id="forward-propagation-of-softmaxloss">forward propagation of SoftMaxLoss</h5>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x, target)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        forward propagation
        &quot;&quot;&quot;</span>
        <span class="hljs-keyword">assert</span> x.dim() == <span class="hljs-number">2</span>, <span class="hljs-string">&quot;dimension of input should be 2&quot;</span>
        exp_x = torch.exp(x)
        y = exp_x / exp_x.sum(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>).expand_as(exp_x)

        <span class="hljs-comment"># parameter &quot;target&quot; is a LongTensor and denotes the labels of classes, here we need to convert it into one hot vectors </span>
        t = torch.zeros(y.size()).type(y.type())
        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(t.size(<span class="hljs-number">0</span>)):
            t[n][target[n]] = <span class="hljs-number">1</span>

        output = (-t * torch.log(y)).sum() / y.size(<span class="hljs-number">0</span>)
        <span class="hljs-comment"># output should be a tensor, but the output of sum() is float</span>
        output = torch.Tensor([output]).type(y.type())
        self.y = y <span class="hljs-comment"># save for backward</span>
        self.t = t <span class="hljs-comment"># save for backward</span>
        <span class="hljs-keyword">return</span> output
</code></pre>
<p>to use the auto-grad scheme of PyTorch, we also define a function to execute the same operation of forward propagation of softmax loss</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SoftmaxLossFunc</span><span class="hljs-params">(x, target)</span>:</span>
    exp_x = torch.exp(x)
    y = exp_x / exp_x.sum(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>).expand_as(exp_x)
    t = torch.zeros(y.size()).type(y.data.type())
    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(t.size(<span class="hljs-number">0</span>)):
        t[n][target.data[n]] = <span class="hljs-number">1</span>

    t = Variable(t)
    output = (-t * torch.log(y)).sum() / y.size(<span class="hljs-number">0</span>)
    <span class="hljs-keyword">return</span> output
</code></pre>
<h5 id="backward-propagation">backward propagation:</h5>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, grad_output)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        backward propagation
        &quot;&quot;&quot;</span>
        grad_input = grad_output * (self.y - self.t) / self.y.size(<span class="hljs-number">0</span>)
        <span class="hljs-keyword">return</span> grad_input, <span class="hljs-keyword">None</span>
</code></pre>
<h4 id="testing-code">testing code:</h4>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_softmax_loss_backward</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;
    analyse the difference between autograd and manual grad
    &quot;&quot;&quot;</span>
    <span class="hljs-comment"># generate random testing data</span>
    x_size = <span class="hljs-number">3200</span>
    x = torch.randn(x_size, x_size) <span class="hljs-comment"># .cuda() # use .cuda for GPU mode</span>
    x_var = Variable(x, requires_grad=<span class="hljs-keyword">True</span>) <span class="hljs-comment"># convert tensor into Variable</span>

    <span class="hljs-comment"># testing labels</span>
    target = torch.LongTensor(range(x_size))
    target_var = Variable(target)

    <span class="hljs-comment"># compute outputs of softmax loss</span>
    y = SoftmaxLoss()(x_var, target_var)

    <span class="hljs-comment"># clone testing data</span>
    x_copy = x.clone()
    x_var_copy = Variable(x_copy, requires_grad=<span class="hljs-keyword">True</span>)

    <span class="hljs-comment"># compute output of softmax loss</span>
    y_hat = SoftmaxLossFunc(x_var_copy, target_var)

    <span class="hljs-comment"># compute gradient of input data with two different method</span>
    y.backward() <span class="hljs-comment"># manual gradient</span>
    y_hat.backward() <span class="hljs-comment"># auto gradient</span>

    <span class="hljs-comment"># compute difference of gradients</span>
    grad_dist = (x_var.grad - x_var_copy.grad).data.abs().sum()
</code></pre>
<p>outputs:</p>
<p>the distance between our implementation and PyTorch auto-gradient is about e-7 under 32 bits floating point precision, and our backward operation is slightly faster than the baseline</p>
<pre><code>=====================================================
|===&gt; testing softmax loss forward
distance between y_hat and y:  0.0
|===&gt; testing softmax loss backward
y:  Variable containing:
 8.5553
[torch.FloatTensor of size 1]

y_hat:  Variable containing:
 8.5553
[torch.FloatTensor of size 1]

x_grad:  Variable containing:
-3.1247e-04  1.3911e-07  4.8041e-07  ...   3.0512e-08  1.7696e-08  1.0826e-07
 7.6744e-07 -3.1246e-04  1.2172e-07  ...   1.2465e-07  6.0764e-08  5.0740e-08
 8.7925e-08  1.7995e-08 -3.1242e-04  ...   1.1499e-07  6.7635e-08  5.2739e-08
                ...                   &#x22F1;                   ...                
 1.0118e-08  1.7118e-07  1.7081e-07  ...  -3.1244e-04  3.1381e-07  2.1709e-08
 2.2232e-07  2.4775e-07  1.0417e-07  ...   4.6105e-08 -3.1172e-04  2.1110e-08
 1.6006e-07  4.8581e-08  3.2675e-08  ...   2.3572e-07  5.3878e-08 -3.1247e-04
[torch.FloatTensor of size 3200x3200]

x_copy.grad:  Variable containing:
-3.1247e-04  1.3911e-07  4.8041e-07  ...   3.0512e-08  1.7696e-08  1.0826e-07
 7.6744e-07 -3.1246e-04  1.2172e-07  ...   1.2465e-07  6.0764e-08  5.0740e-08
 8.7925e-08  1.7995e-08 -3.1242e-04  ...   1.1499e-07  6.7635e-08  5.2739e-08
                ...                   &#x22F1;                   ...                
 1.0118e-08  1.7118e-07  1.7081e-07  ...  -3.1244e-04  3.1381e-07  2.1709e-08
 2.2232e-07  2.4775e-07  1.0417e-07  ...   4.6105e-08 -3.1172e-04  2.1110e-08
 1.6006e-07  4.8581e-08  3.2675e-08  ...   2.3572e-07  5.3878e-08 -3.1247e-04
[torch.FloatTensor of size 3200x3200]

distance between x.grad and x_copy.grad:  1.11203504294e-07
|===&gt; comparing time-costing
time of manual gradient:  1.13225889206
time of auto gradient:  1.40407109261
</code></pre><p>with 64 bits double precision, the difference of gradient is reduced into e-16. Notice that the outputs of two sofmaxloss function have a gap of e-7. Again, our method is slightly faster.</p>
<pre><code>=====================================================
|===&gt; testing softmax loss forward
distance between y_hat and y:  2.31496107617e-07
|===&gt; testing softmax loss backward
y:  Variable containing:
 8.5468
[torch.DoubleTensor of size 1]

y_hat:  Variable containing:
 8.5468
[torch.DoubleTensor of size 1]

x_grad:  Variable containing:
-3.1246e-04  4.7302e-08  2.6106e-08  ...   2.1885e-08  1.5024e-08  6.0311e-09
 4.1688e-08 -3.1245e-04  1.1503e-07  ...   1.8215e-07  3.1857e-08  1.1914e-07
 9.2476e-08  7.1073e-08 -3.1248e-04  ...   2.7795e-08  2.5479e-07  4.8765e-08
                ...                   &#x22F1;                   ...                
 5.0167e-08  1.2661e-07  8.0579e-08  ...  -3.1239e-04  2.0139e-08  1.3870e-08
 2.8047e-07  3.2061e-07  1.8310e-08  ...   1.5054e-08 -3.1248e-04  8.4565e-08
 5.4617e-08  4.3503e-08  5.2926e-08  ...   1.2573e-07  3.3953e-08 -3.1236e-04
[torch.DoubleTensor of size 3200x3200]

x_copy.grad:  Variable containing:
-3.1246e-04  4.7302e-08  2.6106e-08  ...   2.1885e-08  1.5024e-08  6.0311e-09
 4.1688e-08 -3.1245e-04  1.1503e-07  ...   1.8215e-07  3.1857e-08  1.1914e-07
 9.2476e-08  7.1073e-08 -3.1248e-04  ...   2.7795e-08  2.5479e-07  4.8765e-08
                ...                   &#x22F1;                   ...                
 5.0167e-08  1.2661e-07  8.0579e-08  ...  -3.1239e-04  2.0139e-08  1.3870e-08
 2.8047e-07  3.2061e-07  1.8310e-08  ...   1.5054e-08 -3.1248e-04  8.4565e-08
 5.4617e-08  4.3503e-08  5.2926e-08  ...   1.2573e-07  3.3953e-08 -3.1236e-04
[torch.DoubleTensor of size 3200x3200]

distance between x.grad and x_copy.grad:  1.99762357071e-16
|===&gt; comparing time-costing
time of manual gradient:  1.170181036
time of auto gradient:  2.39760398865
</code></pre><h3 id="reference">Reference:</h3>
<p>[1] <a href="http://shuokay.com/2016/07/20/softmax-loss/" target="_blank">http://shuokay.com/2016/07/20/softmax-loss/</a></p>
<p>[2] <a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank">https://en.wikipedia.org/wiki/Cross_entropy</a></p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="gitbook.html" class="navigation navigation-prev " aria-label="Previous page: Install and Use Gitbook on Windows">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="pytorch_autograd.html" class="navigation navigation-next " aria-label="Next page: PyTorch and Automatic Differentiation">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Softmax Loss with Symbolic Differentiation","level":"1.3.2","depth":2,"next":{"title":"PyTorch and Automatic Differentiation","level":"1.3.3","depth":2,"path":"notes/pytorch_autograd.md","ref":"notes/pytorch_autograd.md","articles":[]},"previous":{"title":"Install and Use Gitbook on Windows","level":"1.3.1","depth":2,"path":"notes/gitbook.md","ref":"notes/gitbook.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["mathjax","splitter","back-to-top-button","etoc","-search","-lunr","search-pro"],"pluginsConfig":{"etoc":{"h2lb":3,"header":1,"maxdepth":4,"mindepth":3,"notoc":false},"splitter":{},"search-pro":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"back-to-top-button":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"notes/softmaxloss.md","mtime":"2017-11-22T07:01:37.078Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-11-28T07:10:47.366Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-etoc/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

