{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction 2017年10月7日 启 经过一段时间的思考，也找了挺多的资料，最终没有发现自己想要的网页笔记。主要应该是个人比较懒一点，对这些文件的管理不是很到位，也就希望借助现有的一些工具来实现。不过因为最后没有合适的，所以还是使用gitbook来构建自己的一个空间。 离毕业还剩下一年不到的时间，也趁着这最后的一点时间留一些痕迹。即使是养成一个写博客的习惯也是挺好的，希望能够坚持下去吧。之后回过头看自己写的东西，感觉还是蛮有成就感的。不过考虑到这个空间的开放性，还有自己的一些初衷。博客尽量还是写的好看一点吧，至于语言的话，中英混合，根据需要采用对应的语言，也就不在意这些细节了。 也就不立什么旗帜了，个人随心就好。 "},"Network Quantization/Introduction.html":{"url":"Network Quantization/Introduction.html","title":"Network Quantization","keywords":"","body":"\r Introduction Deep convolutional neural networks (DCNN) have achieved great performance on computer vision tasks, including image classification, object detection and image segmentation. However, it is difficult to deploy deep learning (DL) methods in mobile devices, as deep neural networks always need high computational resource and large storage. Recent studies on adopting DL to hardware mainly focus on two folds: the first one is to design lighter architecture with comparable performance (i.e. SqueezeNet of Han et.al. ) ; the second fold is network quantization, which aims to compress models and speed up computation of DCNN by converting high precision data to low precision version. Other coding methods including Huffman Coding, Compressed Sparse Column (CSC) and Hashing Coding also help reducing size of models. In this paper, we try to compare several quantization and compression methods as shown in follows: Incremental network quantization (INQ), a lossless quantization method with incremental idea. Combined with dynamic network surgery (DNS), the authors compressed AlexNet for 53\\times without accuracy loss Binary weights network (BWN) and XNOR-Net Ternary weights network (TWN) Binarized neural networks (BNN) Trained ternary quantization (TTQ) Deep compression with network quantization, pruning and Huffman code "},"Network Quantization/Incremental Network Quantization.html":{"url":"Network Quantization/Incremental Network Quantization.html","title":"Incremental Network Quantization","keywords":"","body":"Quantization Methods \r INQ (Incremental Network Quantization) Weight Quantization Strategy Proof 1: compute factor 4/3 of $n_1$ Proof 2: compute $n_2$ Weight Partition Strategies Implementation in PyTorch Experimental Results \r INQ (Incremental Network Quantization) As most of the existing methods suffer from high decreasing on model performance and need many training epochs, the authors provided a lossless quantization method to overcome these problems. The proposed method mainly contains three steps: weight partition, group-wise quantization and re-training. Given a trained model, the first step of INQ is to divide weights of the model into to group, one for quantization and another for re-training. Second, apply weight quantization and convert 32-bits floating point data to low precision data. Third, freeze the quantized weights and retraining the network using SGD, then update remaining weights of the network. Repeating these three steps until all weights are quantized, then we can get a low precision model without significant accuracy loss. Considering binary shift operation is more efficient in hardware, the authors quantize weights of convolutional layers and fully connected layers to the power of 2. Weight Quantization Strategy Suppose weights of a pre-trained full precision model can be represented by \\{ W_l: 1 \\le l \\le L \\}, the quantized weights are represented by \\widehat{W_l} , where each item is chosen from P_l = \\{\\pm2^{n_1}, \\cdots, \\pm2^{n_2}, 0\\}. The quantization method is formulated by \\widehat{W_l}(i, j) = \\begin{cases} \\beta sgn(W_l (i, j))&if (\\alpha + \\beta)/2 \\le abs(W_l (i, j)) \\lt 3\\beta/2 &\\\\ 0 & otherwise, \\end{cases} \\tag{1} where \\alpha and \\beta are two adjacent elements in the sorted P_l. Based on Equation (1), n_1 and n_2 in P_l can be computed by n_1 = floor(log_2(4s/3)), where s=max(abs(W_l)); n_2 = n_1+1-2^{(b-1)}/2, where b is the quantized bit-width. Proof 1: compute factor 4/3 of $n_1$ Considering the extremely condition in Equation (1), we have (2^{n_1-1} + 2^{n_1})/2 \\le max(abs(W_l)) \\le3\\cdot2^{n_1}/2. Then, 2^{n_1-1}\\le2max(abs(W_l))/3\\lt2^{n_1}, n_1-1\\le log_2(2max(abs(W_l))/3)\\lt n_1 n_1\\le log_2(4max(abs(W_l))/3)\\lt n_1+1 Because floor(x)\\le x \\le ceil(x), then we let n_1=floor(4max(abs(W_l))/3). For simplifying the equation, define s=max(abs(W_l)), then we have n_1 = floor(log_2(4s/3)). Proof 2: compute $n_2$ As b denotes the expected bit-width, one bit for zero and others for representing the powers of 2, which including 2^{b-1} different values. Here we have (n_1-n_2+1)\\cdot2= 2^{b-1} according to definition of P_l. Thus n_2 can be computed by n_2 = n_1+1-2^{b-1}/2. Weight Partition Strategies In this paper, the authors explored two kinds of weight partition strategies, including random partition and pruning-inspired partition. The second partition strategy considers that weights with larger absolute values are more important than the smaller ones and would have more possibility to be quantized. The experimental results also shows that the pruning-inspired strategy outperforms the first one for about 0.8% with ResNet-18. Implementation in PyTorch Prepare: pre-train a full-precision model Step1, weight partition: decide number of weights to be quantized according to portion \\{\\sigma_1, \\sigma_2, \\cdots, \\sigma_n\\} generate quantization mask T_l(i, j)\\in \\{0,1\\} using pruning-inspired strategy Step2, group-wise quantization: quantize weights according to mask T_l in order to make sure the quantized weights not be changed during re-training phase, here we save weights as \\widehat{W} Step3, re-training: reset learning rate apply forward, backward and computing gradient update weights by using SGD reload quantized weight from \\widehat{W} partially according to mask T_l repeating operations 2 to 4 with full training phase. Repeat: repeat step1 to step 3 until all weights are quantized. Experimental Results The authors adopted the proposed method to several model, including AlexNet, VGG-16, GoogleNet, ResNet-18 and ResNet-50. More experiments for exploration was conducted on ResNet-18. Experimental results on ImageNet using center crop validation are shown as follows. Network Bit-width Top-1/Top-5 Error Decrease in Top-1/Top-5 Error Portion AlexNet ref 32 42.71%/19.77% AlexNet 5 42.61%/19.54% 0.15%/0.23% {0.3, 0.6, 0.8, 1.0} VGG-16 ref 32 31.46%/11.35% VGG-16 5 29.18%/9.70% 2.28%/1.65% {0.5, 0.75, 0.875, 1.0} GoogleNet ref 32 31.11%/10.97% GoogleNet 5 30.98%/10.72% 0.13%/0.25% {0.2, 0.4, 0.6, 0.8, 1.0} ResNet-18 ref 32 31.73%/11.31 ResNet 5 31.02%/10.90% 0.71%/0.41 {0.5, 0.75, 0.875, 1.0} ResNet-50 ref 32 26.78%/8.76% ResNet-50 5 25.19%/7.55% 1.59%/1.21% {0.5, 0.75, 0.875, 1.0} Number of required epochs for training increasing with the expected bit-width going down. The accumulated portions for weight quantization are set as {0.3, 0.5, 0.8, 0.9, 0.95, 1.0}, {0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0}, {0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.975, 1.0} for 4-bits to 2-bits, respectively. Training epochs required for 2-bits finally set to 30 which means that 300 training epochs are required for completing a full quantization procedure. In the other words, the proposed method become time-consuming when the network going deeper. Although the authors convert weights to the powers of 2 and claim that their method would be efficient with binary shift operation in hardware, the computation in there experiments is still using floating operations. Thus they only show the results of model compression instead of speeding up computation. "},"Network Quantization/Dynamic Network Surgery.html":{"url":"Network Quantization/Dynamic Network Surgery.html","title":"Dynamic Network Surgery","keywords":"","body":"DNS (Dynamic Network Surgery) \r Pruning and Splicing important tips Implementation in PyTorch Experimental Results \r In this paper, the authors proposed dynamic network surgery to prune unimportant connections of the network. Different from previous methods, the proposed method contains two operations: pruning and splicing. Considering the complexity of deep neural networks, it is difficult to decide which connection is important and which one should be pruned. Thus the splicing operation is to recover the pruned weights that are found to be important during training phase. In contrary, weights are pruned with no chance to come back in previous methods, and that may lead to severe loss of accuracy. Pruning and Splicing Suppose W_k represents weights of a DNN model, and mask T_k(i,j)\\in \\{0,1\\} denotes the states of connections of network. The optimize object can be formulated as follows: \\min_{W_k, T_k} L(W_k\\odot T_k)~s.t. T_k^{(i, j)}=h_k(W_k^{(i, j)}), \\forall (i,j)\\in \\mathcal{I}, \\tag{1} where L(\\cdot) denotes the loss function, \\odot indicates the Hadamard product operator), \\mathcal{I} represents every entry of weight matrixes and h_k(W_k^{(i,j)}) evaluates the importance of connections in the network. The definition of h_k is shown as follows: h_k(W_k^{(i,j)}) = \\begin{cases} 0 & if~a_k\\gt|W_k^{(i,j)}| \\\\ T_k^{(i,j)} & if~a_k\\le |W_k^{(i,j)}| \\lt b_k \\\\ 1 & if~b_k\\le |W_k^{(i,j)}| \\end{cases}\\tag{2} a_k and b_k in Equation (2) are two thresholds for improving the robustness of the proposed method. Note that if |W_k^{(i,j)}|\\in [a_k, b_k], the state of T_k^{(i,j)} would not be changed. This Equation is applied to the network to re-evaluate all the weights before training phase. Thus the pruned weights can be spliced if they are evaluated as important connections. Further more, the thresholds a_k and b_k are computed by the following equations according to source code on GitHub: a_k = 0.9*(mean(W_k)+cRate*std(W_k)),\\\\ b_k = 1.1*(mean(W_k)+cRate*std(W_k)), \\tag{3} where cRate is the factor deciding the compressing rate of the model. Larger cRate leads to higher compression rate with more loss of accuracy. After T_k is obtained by Equation (2), we can train the network with full training phase and update weights W_k with the following scheme: W_k^{i,j} \\gets W_k^{(i,j)}-\\beta \\frac{\\partial}{\\partial(W_k^{(i,j)}T_k^{(i,j)})}L(W_k\\odot T_k), \\forall(i,j)\\in \\mathcal{I} \\tag{4} Considering the convergence of the proposed method, the authors suggest that the frequencies of pruning and splicing can be slowed down with a monotonically non-increasing function \\sigma(\\cdot) which indicates the probability of mask T_k that to be modified in single training iteration. With amount of training iterations, the value of \\sigma may even decreased to zero, and the pruning and splicing to the network will be stopped. important tips The computation of thresholds only applied at the very beginning, and would not be change after that Considering the gradient vanishing problem, the authors prune convolutional layers and fully connected layers separately, which is similar to Han's paper Pruned weights would also be updated during training Implementation in PyTorch Prepare: pre-train a DNN model Step 1, pruning and splicing: compute a_k and b_k by Equation (3) and update mask T_k by Equation (2) with probability of \\sigma(iter) Step 2, training: adopt forward and backward propagation and compute Loss and Gradient Step 3, updating: update weights of the model with mask T_k using SGD Repeating: repeating step 1 to step 3 until the number of inter reaches its maximum Experimental Results The authors conducted experiments on several models including LeNet-5, LeNet-300-100 and AlexNet. The experimental results can be summarized as follows: Model Top-1 Error Parameters Iterations Compression LeNet-5 reference 0.91% 431K 10K LeNet-5 pruned 0.91% 4.0K 16K 108\\times LeNet-100-300 reference 2.28% 267K 10K LeNet-100-300 pruned 1.99% 4.8K 25K 56\\times AlexNet reference 43.42%/- 61M 450K AlexNet pruned 43.09%/19.99% 3.45M 700K 17.7\\times More detail comparison with work of Han et. al. on AlexNet using single crop validation on ImageNet are shown as follows: Layer Parameters Remaining Parameters Rate of Han et. al.(%) Remaining Parameters Rate(%) conv1 35K ~84% 53.8% conv2 307K ~38% 40.6% conv3 885K ~35% 29.0% conv4 664K ~37% 32.3% conv5 443K ~37% 32.5% fc1 38M ~9% 3.7% fc2 17M ~9% 6.6% fc3 4M ~25% 4.6% Total 61M ~11% 5.7% "},"Network Quantization/Binary Weight Networks.html":{"url":"Network Quantization/Binary Weight Networks.html","title":"Binary Weight Networks","keywords":"","body":"BWN (Binary Weight Networks) and XNOR-Net \r Binary Weight Networks Proof 1: solve objective function Implementation in PyTorch XNOR-Networks Binary Convolution Implementation in PyTorch Experimental Results \r Although deep neural networks have shown great potential in several application domains including computer vision and speech recognition, it is hard to implement DNN methods in hardware with the limitation of storage, compute capabilities and battery power. The authors in this paper proposed two efficient approximation to the neural network: binary weight networks (BWN) and XNOR-Networks. In binary weight networks, all the weights are approximated with binary values. While in XNOR-Networks, both the weights and the inputs to the convolutional layers and fully connected layers are approximated with binary values. The authors also attempted to evaluate their methods on large scale data sets like ImageNet, and proved that their methods outperform baseline for about 16.3%. Source code is available on GitHub. Binary Weight Networks Represent an L-layer DNN model with a triplet . Each element I=\\mathcal{I}_{l(l=1,\\cdots,L)} in \\mathcal{I} is the input tensor of the l^{th} layer, W=\\mathcal{W}_{lk(k=1,\\cdots, K^l)} is the k^{th} weight filter in the l^{th} layer of DNN. * represents convolutional operation with I and W. Note that the authors assume the convolutional layers in the network do not have bias terms. Thus the convolutional operation can be approximated by I*W\\approx(I\\oplus B)\\alpha, where \\oplus indicates a convolution without multiplication, B=\\mathcal{B}_{lk} is a binary filter \\alpha=\\mathcal{A}_{lk} is an scale factor and \\mathcal{W}\\approx\\mathcal{A}_{lk}\\mathcal{B}_{lk}. The optimized objective function is shown as follows: J(B,\\alpha) = \\| W-\\alpha B \\|^2 \\\\ \\alpha^*, \\beta^* = \\arg\\min_{\\alpha, B} J(B,\\alpha) \\tag{1} By solving the objective function mentioned above, we get: B^* = sign(W) \\\\ \\alpha^* = \\frac{1}{n} \\|W\\|_{l1} \\tag{2} Proof 1: solve objective function J(B,\\alpha) = \\| W-\\alpha B \\|^2 = \\alpha^2B^\\intercal B - 2\\alpha W^\\intercal B + W^\\intercal W \\tag{3} Since B\\in\\{+1, -1\\}^n, B^\\intercal B=n are constants, and W^\\intercal W is also constant as W is a known value. Thus the Equation (3) can be re-written as: J(B,\\alpha) =\\alpha^2n-2\\alpha W^\\intercal B+c \\tag{4} Note that \\alpha is a positive value in Equation (4), then the solution of B^* is: B^* = \\arg\\min_B\\{W^\\intercal B\\}~~s.t. B\\in\\{+1, -1\\}^n \\\\ B* = sign(W) \\tag{5} Take the derivative of J, we have: \\frac{\\partial J}{\\partial\\alpha} = 2\\alpha n-2W^\\intercal B = 0 \\\\ \\alpha^* = \\frac{W^\\intercal B^*}{n} = \\frac{W^\\intercal sign(W)}{n} = \\frac{\\sum |W_i|}{n} = \\frac{1}{n}\\|W\\|_{l1} \\tag{6} Implementation in PyTorch Prepare: pre-train a DNN model (the authors did not mention that they use a pre-trained model or training from scratch) Step 1, quantization: quantize weights of convolutional layers using Equation (2) Step 2, training: apply standard forward and backward propagation to the network Step 3, update parameters: update parameters with standard SGD Repeat: repeat step 1 to step 3 until reach max iterations Note that weights in BWN are still floating-point as the author use \\alpha B to approximate W. \\alpha is the mean of weights in every convolutional layer. The main idea of the authors is computing the best approximation of weights. However, optimal approximation to weights do not means optimal approximation to the final outputs of the network. Thus this kinds of methods may still lead to high loss of accuracy. XNOR-Networks Based on the proposed binary weight networks, the authors further explore the method to binarize both weights and inputs. Convolutional operation consist of shift operation and dot product, and if the dot product can be expressed by binary operations, then convolution can be approximated using binary operations. The approximation of dot product between X, W\\in \\mathbb{R}^n can be expressed by X^\\intercal W\\approx \\beta H^\\intercal \\alpha B, where H,B\\in \\{+1,-1\\}^n and \\beta, \\alpha\\in \\mathbb{R}^+, then the optimized objective function is: \\alpha^*,B^*, \\beta^*, H^* = \\arg\\min_{\\alpha, B,\\beta, H} \\| X\\odot W - \\beta\\alpha H\\odot B\\|, \\tag{7} where \\odot indicates the element-wise product. Define Y\\in\\mathbb{R}^n such that Y_i = X_i W_i,~C\\in\\{+1,-1\\}^n such that C_i=H_i B_i and \\gamma\\in\\mathbb{R}^n such that \\gamma=\\beta\\alpha. Equation (7) can be written as: \\gamma^*, C^* = \\arg\\min_{\\gamma,C}\\|Y-\\gamma C\\| \\tag{8} Similar to the Equation (6), the optimal solutions of Equation (8) are shown as follows: C^* = sign(Y) = sign(X)\\odot sign(W) = H^* \\odot B^* \\\\ \\gamma^* = \\frac{\\sum{|Y_i|}}{n}=\\frac{\\sum{|X_i||W_i|}}{n} \\approx(\\frac{1}{n}\\|X\\|_{l1})(\\frac{1}{n}\\|W\\|_{l1}) = \\beta^*\\alpha^* \\\\ H^* = sign(X) \\\\ B^* = sign(W) \\\\ \\beta^*= \\frac{1}{n}\\|X\\|_{l1} \\\\ \\alpha^*= \\frac{1}{n}\\|W\\|_{l1} \\tag{9} Binary Convolution With the binary weights and binary inputs of convolutional layer, the convolution operations can be approximated using binary operations: I*W \\approx (sign(I)\\circledast sign(W)) \\odot K\\alpha \\tag{10} where \\circledast indicates a convolutional operation using XNOR and bit-count operations, K contains scale factors \\beta for all sub-tensors in input I. The authors suggested that applying pooling on binary input results in significant loss of information, thus they changed the standard convolutional blocks to pre-activated version. Binary Gradient: similar to the binarization in the forward pass, the gradient in the backward pass can also be binarized. To preserve the maximum change of gradient, the authors use \\max_i(|g_i^{in}|) as the scale factor. k-bit Quantization: the authors also mentioned that 1-bit quantization of weights can also be converted to k-bit quantization by using q_k(x)=2(\\frac{[(2^k-1)(\\frac{x+1}{2})]}{2^k-1}-\\frac{1}{2} instead of sign(x) function, where [\\cdot] indicates rounding operation and x\\in [-1,1]. Implementation in PyTorch The implementation of XNOR-Net is similar to those of BWN which including three steps: quantization, forward propagation and backward propagation. Experimental Results In this paper, the authors conducted experiments on AlexNet and ResNet and validated the proposed methods using validation data set of ImageNet with single crop. The optimization method used in there experiments is ADAM as it can converge faster and have better performance with binary inputs. Experimental results are shown as follows: Model Top-1 Accuracy/ Top-5 Accuracy Accuracy Loss AlexNet reference 56.6% / 80.2% AlexNet BC 35.4% / 61.0% -21.2% / -19.2% AlexNet BWN 56.8% / 79.4% 0.2% / -0.8% AlexNet BNN 27.9 % / 50.42% -28.7% / -29.78% AlexNet XNOR-Net 44.2% / 69.2% -12.4% / -11% ResNet-18 reference 69.3% / 89.2% ResNet-18 BWN 60.8% / 83.0% -8.5% / -5.8% ResNet-18 XNOR-Net 51.2% / 73.2% -18.1% / -16% GoogLeNet reference 71.3% / 90.0% GoogLeNet BWN 65.5% / 86.1% -5.8% / -3.9% "},"Network Quantization/Ternary Weight Networks.html":{"url":"Network Quantization/Ternary Weight Networks.html","title":"Ternary Weight Networks","keywords":"","body":"TWN (Ternary Weight Networks) \r Training Methods Important Tips Experimental Results \r The authors introduced ternary weight networks (TWNs) to address the limited storage and computational resources issues in hardware. The quantization problem can be formulated as follows: \\begin{cases} \\alpha^*, W^{t*} = &\\arg\\min_{\\alpha, W^t} J(\\alpha, W^t) = \\|W-\\alpha W^t\\|_2^2 \\\\ s.t. & a\\ge0, W_i^t\\in\\{-1,0,1\\}, i=1,2,\\dots, n. \\end{cases} \\tag{1} Here n is the size of filter, W represents weights of the network. With W\\approx \\alpha W^t and assuming the convolutional layer do not have bias term, forward propagation of ternary weight networks is as follows: \\begin{cases} Z & = &X*W \\approx X*(\\alpha W^t) = (\\alpha X)\\oplus W^t \\\\ X^{next} & = & g(Z) \\end{cases} \\tag{2} where X indicates inputs, * indicates convolutional operation, g is the non-linear activation function, \\oplus indicates the inner product or convolutional operation without any multiplication, X^{next} indicates the outputs. The approximated solution of W with threshold-based ternary function is as follows: W_i^t = f_t(W_i|\\triangle) = \\begin{cases} +1, if~W_i \\gt \\triangle \\\\ 0, if~|W_i| \\le \\triangle \\\\ -1, if~W_i \\lt -\\triangle \\end{cases} \\tag{3} The optimized objective function can be written as: \\alpha^*, \\triangle^* = \\arg\\min_{\\alpha\\ge0, \\triangle\\gt 0}(|I_\\triangle|\\alpha^2-2(\\sum_{i\\in I_\\triangle}|W_i|)\\alpha+c_\\triangle) \\tag{4} where I_\\triangle = \\{i|~|W|\\gt\\triangle\\} and |I_\\triangle| denotes the number of elements in I_\\triangle; c_\\triangle = \\sum_{i\\in I_\\triangle^c}W_i^2 is a \\alpha-independent constant. Thus the optimal solutions of the objective function can be computed as follows: \\alpha_\\triangle^* = \\frac{1}{|I_\\triangle|}\\sum_{i\\in I_\\triangle}|W_i| \\\\ \\triangle^* = \\arg\\max_{\\triangle\\gt 0}(\\sum_{i\\in I_\\triangle}|W_i|^2) \\tag{5} Here solution of \\triangle is approximated by \\triangle^*\\approx0.7\\cdot E(|W|) \\approx \\frac{0.7}{n}\\sum_{i=1}^n|W_i|. Training Methods The training of ternary weight networks can be summarized to three steps: quantization, training and updating. Quantization phase is to quantize the weights of convolutional layers using Equation (5), then apply standard forward and backward propagation to the network, and update parameters using standard SGD. Source code is available on GitHub. Important Tips \\alpha is used as the scaling factor for input X not for weights W gradient is computed using W^t quantized weights are used during forward and backward but not during parameters update, so W_l^r \\gets W_l^r - \\eta \\frac{\\partial C}{\\partial W_l^t} first compute \\triangle then compute mask, finally compute \\alpha \\triangle is computed with all |W^r| while \\alpha is computed only with those |W^r|>\\triangle apply weight_decay would lead results worse this blog is useful for implementation Experimental Results Three data sets are used in this paper, including MNIST, CIFAR-10, ImageNet. To different data sets, the authors conducted experiments using LeNet-5 (32-C5 + MP2 + 64-C5 + MP2 + 512FC + SVM), VGG-inspired network (2\\times(128-C3) + MP2 + 2\\times(256-C3) + MP2 + 2\\times(512-C3) + MP2 + 1024-FC + Softmax), ResNet-18, respectively. Network architecture and parameters setting for different data sets are shown as follows: MNIST CIFAR-10 ImageNet network architecture LeNet-5 VGG-7 ResNet-18 (B) weight decay 1e-4 1e-4 1e-4 mini-batch size of BN 50 100 64(\\times4 GPUs) initial learning rate 0.01 0.1 0.1 learning rate decay (divided by 10) epochs 15, 25 80, 120 30, 40, 50 momentum 0.9 0.9 0.9 Comparison of the proposed method and the previous methods are shown as follows: Method MINIST CIFAR-10 ImageNet Top1 (ResNet-18 / ResNet-18B) ImageNet Top5 (ResNet-18 / ResNet-18B) TWN 99.35 92.56 61.8 / 65.3 84.2 / 86.2 BPWN 99.05 90.18 57.5 / 61.6 81.2 / 83.9 FPWN (full precision) 99.41 92.88 65.4 / 67.6 86.76 / 88.0 Binary Connect 98.82 91.73 - - Binarized Neural Networks 88.6 89.85 - - Binary Weight Networks - - 60.8 83.0 XNOR-Net - - 51.2 73.2 "},"Network Quantization/Trained Ternary Quantization.html":{"url":"Network Quantization/Trained Ternary Quantization.html","title":"Trained Ternary Quantization","keywords":"","body":"TTQ (Trained Ternary Quantization) The ternary quantization methods proposed in this paper based on threshold and quantized weights to 0 and {-1, +1} with two different scaling factor. The authors also suggested the rule of scaled gradient to update weight in different group. Quantized ternary weight w_l^i of the network can be calculated by: w_l^t = \\begin{cases} W_l^p&:& \\tilde{w_l} \\gt \\triangle_l \\\\ 0&:& |\\tilde{w_l}| \\le \\triangle_l \\\\ -W_l^n&:& \\tilde{w_l} \\lt -\\triangle_l \\end{cases} \\tag{1} Different from previous work where weights are calculated from 32-bit weights, the scaling coefficients W_l^p, W_l^n are two independent parameters and are trained with gradient descent: \\frac{\\partial L}{\\partial W_l^p} = \\sum_{i\\in I_l^p}\\frac{\\partial L}{\\partial w_l^t(i)}, \\\\ \\frac{\\partial L}{\\partial W_l^n} = \\sum_{i\\in I_l^n}\\frac{\\partial L}{\\partial w_l^t(i)}, \\tag{2} Here I_l^p=\\{i|\\tilde{w_l}(i) \\gt \\triangle_l\\} and I_l^n=\\{i|\\tilde{w_l}(i) \\lt -\\triangle_l\\}. The proposed scaled gradients for 32-bit are weights computed by: \\frac{\\partial L}{\\partial\\tilde{w_l}} = \\begin{cases} W_l^p\\times\\frac{\\partial L}{\\partial w_l^t}&:&\\tilde{w} \\gt \\triangle_l \\\\ 1\\times\\frac{\\partial L}{\\partial w_l^t}&:&|\\tilde{w}| \\le \\triangle_l \\\\ W_l^n\\times\\frac{\\partial L}{\\partial w_l^t}&:&\\tilde{w} \\lt -\\triangle_l \\end{cases} \\tag{3} Different from TWN, the authors explored two strategies to decide the values of thresholds: (1) \\triangle_l=t\\times\\max(|\\tilde{w}|), t is set to 0.05 on CIFAR-10 and ImageNet; (2) \\triangle_l=r, where r is a hyper-parameter and adjusted with various sparsities. For exploring the trade-off between sparsity and accuracy, the sparsity of weights are growing from 0 to 0.5 and the best result occurs with sparsity between 0.3 and 0.5. Training Methods First: normalize full-precision weights to range [-1, +1] by dividing each weight by the maximum weight Second: quantize weights to \\{-W_l^n, 0, W_l^p\\} with threshold factor t Third: compute the scaled gradients and update the scaling coefficients with back propagation Source code is available on GitHub. Experimental results Testing error on CIFAR-10 with ResNet is shown as follows: Model Full precision Trained Ternary Quantization Improvement ResNet-20 8.23 8.87 -0.64 ResNet-32 7.67 7.63 0.04 ResNet-44 7.18 7.02 0.16 ResNet-56 6.80 6.44 0.36 The authors further evaluate their method on ImageNet with AlexNet and ResNet-18B. To AlexNet, the authors preserves full precision weights in first convolutional layer and the last fully-connected layer, and other layer parameters are quantized to ternary values. Experimental results are shown as follows: Model Bit-width ImageNet Top-1/Top-5 Error Accuracy Loss AlexNet reference 32 42.8 / 19.7 - AlexNet DoReFa 1 46.1 / 23.7 -3.3 / -4.0 AlexNet TWN 2 45.5 / 23.2 -2.7 / -3.5 AlexNet TTQ 2 42.5 / 20.3 -0.3 / -0.6 ResNet-18B reference 32 30.4 / 10.8 - ResNet-18B DoReFa 1 39.2 / 17.0 -8.8 / -6.2 ResNet-18B TWN 2 34.7 / 13.8 -4.3 / -3.0 ResNet-18B TTQ 2 33.4 / 12.8 -3.0 / -2.0 "},"Network Quantization/Binarized Neural Networks.html":{"url":"Network Quantization/Binarized Neural Networks.html","title":"Binarized Neural Networks","keywords":"","body":"BNN (Binarized Neural Networks) \r Proposed Method Binarization Strategies Gradient Shift-based Batch Normalization Shift-based AdaMax Binarized Input Training Method Experimental Results Extension of BNN Experimental Results \r In this paper, the authors proposed a method to train Binarized Neural Networks (BNNs), a network with binary weights and activations. The proposed BNNs drastically reduce the memory consumption (size and number of accesses) and have higher power-efficiency as it replaces most arithmetic operations with bit-wise operations. The code implemented in Theano and Torch is available on GitHub. Proposed Method Binarization Strategies Constrain both weights and activation to either +1 or -1 has higher efficiency in hardware. The authors discussed two binarization functions including deterministic and stochastic. Formulation of deterministic binarization function is: x^b = sign(x)= \\begin{cases} +1 & if ~x\\ge 0 \\\\ -1 & otherwise, \\end{cases} \\tag{1} The stochastic binarization function is: x^b = \\begin{cases} +1, & \\mathrm{with~probability}~p=\\sigma(x) \\\\ -1, & \\mathrm{with~probability}~1-p, \\end{cases} \\tag{2} where \\sigma is the \"hard sigmoid\" function: \\sigma(x) = clip(\\frac{x+1}{2},0,1) = \\max(0,\\min(1,\\frac{x+1}{2})) \\tag{3} The authors suggested that the stochastic binarization is harder to implement as it requires the hardware to generate random bits, though it is more appealing than the deterministic binarization, so they preferred to use the deterministic binarization function in their experiments. Gradient Real-valued gradients are computed and accumulated in real-valued variables in this paper, as high precision is required for SGD. Previous work shows that using \"straight-through estimator\" can help the network training faster, the authors used straight-through estimator of \\frac{\\partial C}{\\partial r} simplified as: g_r = g_q1_{|r|\\le1} \\tag{4} which cancels the gradient when r is too large. The derivation 1_{|r|\\le1} can also be seen as propagating the gradient through hard tanh: \\mathrm{Htanh}(x)=clip(x,-1,1)=\\max(-1,\\min(1,x)) \\tag{5} The real-valued weights w^r first projected to [-1,+1] and then quantized to binarized weights w^b using w^b=sign(w^r). Shift-based Batch Normalization The authors proposed a shift-based batch normalization (SBN) to achieve the results of BN so as to speed up computation of batch normalization. The algorithm is shown as follows: \\mu_B \\gets \\frac{1}{m}\\sum_{i=1}^m x_i \\\\ C(x_i) \\gets (x_i-\\mu_B) \\\\ \\sigma^2_B \\gets \\frac{1}{m} \\sum_{i=1}^m (C(x_i)\\ll\\gg AP2(C(x_i))) \\\\ \\hat{x_i} \\gets C(x_i) \\ll \\gg AP2((\\sqrt{\\sigma^2_B+\\epsilon})^{-1}) \\\\ y_i \\gets AP2(\\gamma) \\ll \\gg \\hat{x_i} \\tag{6} Where AP2 is the approximate power-of-2, \\ll\\gg indicates both left and right binary shift operations. Shift-based AdaMax Since ADAM requires many multiplications, the authors suggested to use shift-based AdaMax which is shown as follows: m_t \\gets \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\ v_t \\gets \\max(\\beta_2 \\cdot v_{t-1}, g|t|) \\\\ \\theta_t \\gets \\theta_{t-1} - (\\alpha \\ll \\gg (1-\\beta_1)) \\cdot \\hat{m} \\ll \\gg v_t^{-1} \\tag{7} Where g_t^2 indicates the element-wise square g_t\\circ g_t. Good default setting are \\alpha=2^{-10},1-\\beta_1=2^{-3},1-\\beta_2=2^{-10}. All operations on vectors are element-wise and \\beta_1^t, \\beta_2^t denote \\beta_1 and \\beta_2 to the power t. Binarized Input Since the input representation has much fewer channels than the internal representations in computer vision and it is easy to convert continuous-valued inputs to fixed point numbers, the authors suggested to compute output of first layer by: s=x \\cdot w^b \\\\ s=\\sum_{n=1}^8 2^{n-1}(x^n \\cdot w^b) \\tag{8} where x is a vector of 1024 8-bit inputs, x_1^8 is the most significant bit of the first input, w^b is a vector of 1024 1-bit weights and s is the resulting weighted sum. Training Method Step 1, forward: binarized weights and apply SBN Step 2, backward: compute real-valued gradient g_a with constraint descripted in Equation (4), and compute gradient of weights Step 3, update: update weights with constraint descripted in Equation (4) Repeating: repeating step 1 to step 3, until finish the training. Experimental Results The authors evaluated their method on three data sets including MNIST, SVHN and CIFAR-10, results are shown as follows: Method MNIST SVHN CIFAR-10 BNN Torch7 1.40% 2.53% 10.15% BNN Theano 0.96% 2.80% 11.40% Extension of BNN Following the work of BNN, the authors proposed a training method to improve performance of BNN in four folds: (1) using low learning rate (the authors suggested to use the learning rate of 1e-4); (2) using PReLU instead of ReLU to absorb the scaling factor for weights to the activation function; (3) introducing a regularization term to the loss function to encourage the weights to be bipolar; (4) using scale layer in fully connected layer to bring the outputs to normal. The regularization term introduced in this paper is formulated by: J(W,b) = L(W,b)+\\lambda \\sum_{l=1}^L \\sum_{i=1}^{N_l} \\sum_{j=1}^{M_l} (1-(W_{l,ij})^2) \\tag{9} To improve the accuracy, the authors used multiple binarizations for the activation: A_l \\approx \\sum_{i=1}^m (\\alpha_{l,i} H{l,i}) \\tag{10} For i=1, H_{l,1} is the sign of A_l and \\alpha_{l,i} is the average absolute value of A_l, for i\\gt 1, H_{l,i} and \\alpha_{l,i} is calculated in the way based on residual approximation error from step i-1: E_{L,I} = a_l-\\sum_{j=1}^{i-1}\\alpha_{l,j}\\ast H_{l,j}. So the output O_l is calculated by: O_l = W_l \\cdot A_{l-1} \\approx \\sum_{i=1}^m (\\alpha_{l-1,i}xnor-popcnt(B_l, H_{l-1,i})) \\tag{11} Experimental Results The authors conducted experiments on ImageNet with AlexNet and NIN, the results are shown as follows: Method Bits of Activation Precision of Last Layer Compression Rate Accuracy AlexNet BNN 1 Full 10.3\\times 50.4/ 27.9 AlexNet XNOR-net 1 Full 10.3\\times 69.2 / 44.2 AlexNet DoReFa 2 Full 10.3\\times - / 49.8 AlexNet Extended-BNN 2 Binary 31.2\\times 71.1 / 46.6 NIN Extended-BNN 2 Binary 23.6 \\times 75.6 / 51.4 "},"Network Quantization/Deep Compression.html":{"url":"Network Quantization/Deep Compression.html","title":"Deep Compression","keywords":"","body":"Deep Compression In this paper the authors introduces \"deep compression\" to compress model size of deep convolutional neural networks. The proposed method consists of three stage: pruning, trained quantization and Huffman coding. The authors first prunes weights by learning the important connections. Second, quantize weights to enforce weight sharing. Third, apply Huffman coding to reduce storage further. Proposed Methods Network Pruning: First, learn connectivity by normal network training. Second, remove weights below threshold. Third, retrain the network to learn the final weights for remaining sparse connections. After pruning, the sparse structure is stored using compress sparse row (CSR) or compress sparse column (CSC). Trained Quantization: use k-clusters with b-bits to represent n-connections of the network. The compression rate can be computed by: r=\\frac{nb}{nlog_2(k)+kb} \\tag{1} The authors use k-means algorithm to get k-clusters C=\\{c_1, c_2,\\dots,c_k\\} to represent n original weights W=\\{w_1, w_2, \\dots,w_n\\}, and the optimized objective function is: \\arg\\min_C\\sum_{i=1}^{k}\\sum_{w\\in c_i} |w-c_i|^2 \\tag{2} As the initialization of k-means algorithm is quite important, the authors explored three initial methods including forgy (random), density-based, linear. The authors suggested that using linear initialization can get better result as it has better representation to the few large weights which are important to the networks. Once the centroids of weights are decided by k-means clustering, the index of sharing weight table is stored for each connections and used when conducting forward or backward propagation. Gradients for the shared weights are computed by: \\frac{\\partial \\mathcal{L}}{\\partial C_k} = \\sum_{i,j}\\frac{\\partial \\mathcal{L}}{\\partial W_{i,j}}\\frac{\\partial W_{i,j}}{\\partial C_k} = \\sum_{i,j}\\frac{\\partial{\\mathcal L}}{\\partial W_{i,j}}\\mathbb{1}(I_{i,j}=k), \\tag{3} where \\mathcal{L} indicates loss, W_{i,j} indicates weight in the i-th column and j-th row, C_k indicates the k-th centroid of the layer and \\mathbb{1}(\\cdot) is an indicator function. Experimental Results The authors conducted experiments on two data sets: on MNIST, they used LeNet-300-100 and LeNet-5 while used AlexNet and VGG-16 on ImageNet to evaluate the proposed methods. Results on MINIST are summarized as follows: Model Top-1 (/ Top-5 Error) Accuracy Loss Parameters Compression Rate LeNet-300-100 reference 1.64% - 1070 KB - LeNet-300-100 compressed 1.58% 0.06% 27 KB 40 \\times LeNet-5 reference 0.80% - 1720 KB - LeNet-5 compressed 0.74% 0.06% 44 KB 39 \\times AlexNet reference 42.78% / 19.73% - 240 MB - AlexNet compressed 42.78% / 19.70% 0% / 0.03% 6.9 MB 35 \\times VGG-16 reference 31.50% / 11.32% - 552 MB - VGG-16 compressed 31.17% / 10.91% 0.33% / 0.41% 11.3 MB 49 \\times "},"notes/gitbook.html":{"url":"notes/gitbook.html","title":"Install and Use Gitbook on Windows","keywords":"","body":"gitbook 安装及使用（windows）参考 \r 安装node.js 使用npm安装gitbook gitbook使用 常用命令 构建github blog gitbook 插件 支持中文搜索的search-pro \r 安装node.js 直接到官网下载exe安装即可，参考网站 使用npm安装gitbook 注意：不知道什么原因，gitbook命令只能在nodejs的终端中使用，在系统的cmd或者powershell中不被识别 npm install gitbook-cli -g 在node.js command prompt 中使用 gitbook -V 确认安装成功 gitbook使用 打开node.js command prompt 运行gitbook 常用命令 本地预览： gitbook serve ./book_name 输出静态网站： gitbook build ./book_path ./output_path 查看帮助： gitbook help 构建github blog 注意： 仓库名称必须为usename.github.io，注意大小写，跟官方教程有点不一样，这个后期再弄。参考 创建github远程仓库 克隆仓库到本地： git clone .... 创建一个新的分支 git checkout -b gh-pages(分支名必须为gh-pages) 将分支push到仓库 切换到主分支master 将gitbook build得到的静态网页文件复制到改仓库的本地目录下 提交到远程仓库 打开http://username.github.io可以看到网页版本的gitbook gitbook 插件 支持中文搜索的search-pro github: https://github.com/gitbook-plugins/gitbook-plugin-search-pro 在终端运行命令： npm install gitbook-plugin-search-pro 修改book.json: { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ] } "},"notes/softmaxloss.html":{"url":"notes/softmaxloss.html","title":"Softmax Loss with Symbolic Differentiation","keywords":"","body":"Manual Gradient V.S. Auto Gradient (PyTorch) \r Reproduction of Softmax Loss with Cross Entropy softmax function Cross entropy loss Coding in PyTorch Using basic function of PyTorch testing code: Reference: \r github: https://github.com/ICEORY/softmax_loss_gradient.git Reproduction of Softmax Loss with Cross Entropy softmax function the softmax function is defined by \r y_i = \\frac{e^{x_i}}{\\sum e^{x_k}}, for~i=1,..., C\r where $x$ is the input with $C$ channels, $y$ is the respected output. the gradient of softmax $\\frac{\\partial y_i}{\\partial x_j}$ is computed by: \r {\\rm if}~i=j,~\\frac{\\partial y_i}{\\partial x_j}=\\frac{\\partial y_i}{\\partial x_i} = \\frac{e^{x_j}\\cdot \\sum e^{x_k}-e^{x_i}\\cdot e^{x_i}}{(\\sum e^{x_k})^2} = \\frac{e^{x_i}}{\\sum e^{x_k}}\\frac{\\sum e^{x_k}-e^{x_i}}{\\sum e^{x_k}} = y_i \\cdot (1-y_i)\r \r {\\rm if}~i\\ne j,~\\frac{\\partial y_i}{\\partial x_j}=\\frac{\\partial \\frac{e^{x_j}}{\\sum e^k}}{\\partial x_j} = \\frac{0\\cdot \\sum e^{x_k}-e^{x_i}\\cdot e^{x_j}}{(\\sum e^{x_k})^2} = -\\frac{e^{x_i}}{\\sum e^{x_k}} \\frac{e^{x_j}}{\\sum e^{x_k}} = -y_i \\cdot y_j\r Cross entropy loss the cross entropy loss function is \r \\mathcal L = -\\sum_{c=1}^C t_c\\cdot log(y_c),\r where $t$ is the one-hot label. For a batch of samples, the cross-entropy loss can be re-written to \r \\mathcal L = -\\sum_{n=1}^N \\sum_{c=1}^C t_{nc} \\cdot log(y_{nc})\r the gradient of cross entropy loss is computed by \r \\frac{\\partial \\mathcal L}{\\partial x_i} = -\\sum_{c=1}^C \\frac{\\partial t_c log (y_c)}{\\partial x_i} = -\\sum_{c=1}^C t_c \\frac{\\partial log(y_c)}{\\partial x_i} = -\\sum_{c=1}^C t_c\\frac{1}{y_c}\\frac{\\partial y_c}{\\partial x_i} = -\\frac{t_i}{y_i}\\frac{\\partial {y_i}}{\\partial x_i}-\\sum_{i\\ne j}^C \\frac{t_j}{y_j}\\frac{\\partial y_j}{\\partial x_i} = -\\frac{t_i}{y_i}y_i(1-y_i) - \\sum_{i\\ne j}^C \\frac{\\partial t_j}{y_j}(-y_i y_j) = -t_i+t_i y_i + \\sum_{i\\ne j}^C t_j y_i = -t_i +y_i\\sum_i^C t_i = y_i-t_i\r Coding in PyTorch Using basic function of PyTorch forward propagation of SoftMaxLoss def forward(self, x, target): \"\"\" forward propagation \"\"\" assert x.dim() == 2, \"dimension of input should be 2\" exp_x = torch.exp(x) y = exp_x / exp_x.sum(1).unsqueeze(1).expand_as(exp_x) # parameter \"target\" is a LongTensor and denotes the labels of classes, here we need to convert it into one hot vectors t = torch.zeros(y.size()).type(y.type()) for n in range(t.size(0)): t[n][target[n]] = 1 output = (-t * torch.log(y)).sum() / y.size(0) # output should be a tensor, but the output of sum() is float output = torch.Tensor([output]).type(y.type()) self.y = y # save for backward self.t = t # save for backward return output to use the auto-grad scheme of PyTorch, we also define a function to execute the same operation of forward propagation of softmax loss def SoftmaxLossFunc(x, target): exp_x = torch.exp(x) y = exp_x / exp_x.sum(1).unsqueeze(1).expand_as(exp_x) t = torch.zeros(y.size()).type(y.data.type()) for n in range(t.size(0)): t[n][target.data[n]] = 1 t = Variable(t) output = (-t * torch.log(y)).sum() / y.size(0) return output backward propagation: def backward(self, grad_output): \"\"\" backward propagation \"\"\" grad_input = grad_output * (self.y - self.t) / self.y.size(0) return grad_input, None testing code: def test_softmax_loss_backward(): \"\"\" analyse the difference between autograd and manual grad \"\"\" # generate random testing data x_size = 3200 x = torch.randn(x_size, x_size) # .cuda() # use .cuda for GPU mode x_var = Variable(x, requires_grad=True) # convert tensor into Variable # testing labels target = torch.LongTensor(range(x_size)) target_var = Variable(target) # compute outputs of softmax loss y = SoftmaxLoss()(x_var, target_var) # clone testing data x_copy = x.clone() x_var_copy = Variable(x_copy, requires_grad=True) # compute output of softmax loss y_hat = SoftmaxLossFunc(x_var_copy, target_var) # compute gradient of input data with two different method y.backward() # manual gradient y_hat.backward() # auto gradient # compute difference of gradients grad_dist = (x_var.grad - x_var_copy.grad).data.abs().sum() outputs: the distance between our implementation and PyTorch auto-gradient is about e-7 under 32 bits floating point precision, and our backward operation is slightly faster than the baseline ===================================================== |===> testing softmax loss forward distance between y_hat and y: 0.0 |===> testing softmax loss backward y: Variable containing: 8.5553 [torch.FloatTensor of size 1] y_hat: Variable containing: 8.5553 [torch.FloatTensor of size 1] x_grad: Variable containing: -3.1247e-04 1.3911e-07 4.8041e-07 ... 3.0512e-08 1.7696e-08 1.0826e-07 7.6744e-07 -3.1246e-04 1.2172e-07 ... 1.2465e-07 6.0764e-08 5.0740e-08 8.7925e-08 1.7995e-08 -3.1242e-04 ... 1.1499e-07 6.7635e-08 5.2739e-08 ... ⋱ ... 1.0118e-08 1.7118e-07 1.7081e-07 ... -3.1244e-04 3.1381e-07 2.1709e-08 2.2232e-07 2.4775e-07 1.0417e-07 ... 4.6105e-08 -3.1172e-04 2.1110e-08 1.6006e-07 4.8581e-08 3.2675e-08 ... 2.3572e-07 5.3878e-08 -3.1247e-04 [torch.FloatTensor of size 3200x3200] x_copy.grad: Variable containing: -3.1247e-04 1.3911e-07 4.8041e-07 ... 3.0512e-08 1.7696e-08 1.0826e-07 7.6744e-07 -3.1246e-04 1.2172e-07 ... 1.2465e-07 6.0764e-08 5.0740e-08 8.7925e-08 1.7995e-08 -3.1242e-04 ... 1.1499e-07 6.7635e-08 5.2739e-08 ... ⋱ ... 1.0118e-08 1.7118e-07 1.7081e-07 ... -3.1244e-04 3.1381e-07 2.1709e-08 2.2232e-07 2.4775e-07 1.0417e-07 ... 4.6105e-08 -3.1172e-04 2.1110e-08 1.6006e-07 4.8581e-08 3.2675e-08 ... 2.3572e-07 5.3878e-08 -3.1247e-04 [torch.FloatTensor of size 3200x3200] distance between x.grad and x_copy.grad: 1.11203504294e-07 |===> comparing time-costing time of manual gradient: 1.13225889206 time of auto gradient: 1.40407109261 with 64 bits double precision, the difference of gradient is reduced into e-16. Notice that the outputs of two sofmaxloss function have a gap of e-7. Again, our method is slightly faster. ===================================================== |===> testing softmax loss forward distance between y_hat and y: 2.31496107617e-07 |===> testing softmax loss backward y: Variable containing: 8.5468 [torch.DoubleTensor of size 1] y_hat: Variable containing: 8.5468 [torch.DoubleTensor of size 1] x_grad: Variable containing: -3.1246e-04 4.7302e-08 2.6106e-08 ... 2.1885e-08 1.5024e-08 6.0311e-09 4.1688e-08 -3.1245e-04 1.1503e-07 ... 1.8215e-07 3.1857e-08 1.1914e-07 9.2476e-08 7.1073e-08 -3.1248e-04 ... 2.7795e-08 2.5479e-07 4.8765e-08 ... ⋱ ... 5.0167e-08 1.2661e-07 8.0579e-08 ... -3.1239e-04 2.0139e-08 1.3870e-08 2.8047e-07 3.2061e-07 1.8310e-08 ... 1.5054e-08 -3.1248e-04 8.4565e-08 5.4617e-08 4.3503e-08 5.2926e-08 ... 1.2573e-07 3.3953e-08 -3.1236e-04 [torch.DoubleTensor of size 3200x3200] x_copy.grad: Variable containing: -3.1246e-04 4.7302e-08 2.6106e-08 ... 2.1885e-08 1.5024e-08 6.0311e-09 4.1688e-08 -3.1245e-04 1.1503e-07 ... 1.8215e-07 3.1857e-08 1.1914e-07 9.2476e-08 7.1073e-08 -3.1248e-04 ... 2.7795e-08 2.5479e-07 4.8765e-08 ... ⋱ ... 5.0167e-08 1.2661e-07 8.0579e-08 ... -3.1239e-04 2.0139e-08 1.3870e-08 2.8047e-07 3.2061e-07 1.8310e-08 ... 1.5054e-08 -3.1248e-04 8.4565e-08 5.4617e-08 4.3503e-08 5.2926e-08 ... 1.2573e-07 3.3953e-08 -3.1236e-04 [torch.DoubleTensor of size 3200x3200] distance between x.grad and x_copy.grad: 1.99762357071e-16 |===> comparing time-costing time of manual gradient: 1.170181036 time of auto gradient: 2.39760398865 Reference: [1] http://shuokay.com/2016/07/20/softmax-loss/ [2] https://en.wikipedia.org/wiki/Cross_entropy "},"notes/pytorch_autograd.html":{"url":"notes/pytorch_autograd.html","title":"PyTorch and Automatic Differentiation","keywords":"","body":"Automatic Differentiation of PyTorch \r 自动求导概述 自动求导 PyTorch 自动求导 Reference: \r 自动求导概述 参考文献： Automatic Differentiation in Machine Learning: a Survey 目前用于求导的方法大致可以分为四类： 手动解析并代码实现 数值求导 (numerical differentiation), 使用有限的微分近似 符号求导 (symbolic differentiation), 使用代数表达式进行计算 自动求导 (automatic differentiation, also called algorithmic differentiation) 自动求导与数值求导、符号求导两者都不同，如果没有详细的了解的话，容易将自动求导归为数值求导。事实上，自动求导只提供一个数值的结果，并不能像符号求导一样提供一个完整的表达式，但它也确实是根据符号求导的规则进行，通过反向追踪表达式的运算过程实现求导。因此，自动求导兼具了数值求导与符号求导两者的特性。 数值求导的表达式如下： \r \\frac{\\partial f({\\bold x})}{\\partial x_i} \\approx \\frac{f(x-h {\\bold e}_i)-f({\\bold x})}{h},\r 其中 ${\\bold e}_i$ 是第 $i$ 单位向量，$h \\gt 0$ 是一个小的步长。 符号求导主要根据一系列的求导规则进行变换，例如 \r \\frac{d}{dx}(f(x)+g(x)) \\leadsto \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)\r 或 \r \\frac{d}{dx}(f(x)g(x)) \\leadsto (\\frac{d}{dx}f(x))g(x)+f(x)(\\frac{d}{dx}g(x))\r 从优化的角度，符号求导可以给出问题的内部结构，并用于结果分析。然而，符号求导也容易得到冗长的符号表达式，导致计算困难。 自动求导 自动求导依赖于所有的数值计算都是由有限的元操作构成，并且这些元操作的求导是已知的。依据链式法则，可以将所有组成操作的求导过程联系起来，完成整体的求导。自动求导的模式包括：前向模式以及反向模式。 以 $f(x_1,x_2)={\\rm ln}(x_1)+x_1x_2-{\\rm sin}(x_2)$ 的求导为例子。其计算过程可以由下图表示。 前向模式求解过程： 反向模式求解过程： PyTorch 自动求导 在PyTorch中的自动求导是tape-based autograd，换句话说是基于类似反向模式的自动求导。通过动态的构建运算图，然后反向传播对各个成分进行求导。在PyTorch中，基本的元操作的求导过程已经写好在程序中，如： class Sinh(Function): @staticmethod def forward(ctx, i): ctx.save_for_backward(i) return i.sinh() @staticmethod def backward(ctx, grad_output): i, = ctx.saved_variables return grad_output * i.cosh() 其中Function是一个重要的类，所有元操作或者需要自定义求导过程的操作都从Function类继承，可以记录操作的轨迹并用于自动求导的过程。 PyTorch动态图构建过程如下图所示： Reference: [1] https://justindomke.wordpress.com/2009/03/24/a-simple-explanation-of-reverse-mode-automatic-differentiation/ [2] https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/ [3] python autograd tool: https://github.com/HIPS/autograd [4] https://github.com/pytorch/pytorch/tree/v0.2.0 "},"notes/caffe2.html":{"url":"notes/caffe2.html","title":"Install Caffe2 on Windows","keywords":"","body":"Caffe2 reference [1] https://caffe2.ai/docs/getting-started.html?platform=windows&configuration=compile [2] http://research.wmz.ninja/articles/2017/05/build-caffe2-on-windows-10-gpu.html "},"notes/markdown.html":{"url":"notes/markdown.html","title":"Markdown Tutorial","keywords":"","body":"markdown \r markdown 基本教程 markdown mermaid 其他基本语法： 形状 连接方式 markdown latex \r markdown 基本教程 参考 [1]：http://itmyhome.com/markdown/article/extension/task-list.html 主要包括 markdown 的基本语法、扩展语法、编辑器以及扩展、格式转换等方面的内容； markdown mermaid 参考 [1] https://mermaidjs.github.io/flowchart.html mermaid 是 markdown 中用于画图的工具，基本语法如下： graph TD start-->stop graph LR start-->stop 其中 graph TD 表示画图的方向，表示方向的语法： TB - top bottom BT - bottom top RL - right left LR - left right TD - same as TB 其他基本语法： 形状 带文字的节点 graph LR id1[this is a node] 带圆角的节点 graph LR id1(this is a node with round edges) 圆形的节点 graph LR id1((this is a circle)) 不对称形状：id1>This is the text in the box] 斜方形：id1{This is the text in the box} 连接方式 实线箭头： A-->B 实线连接： A --- B 带文字连接： A-- This is the text ---B 或者 A---|This is the text|B 带文字箭头连接： A-->|text|B A-- text -->B 虚线连接： A-.->B; 粗箭头： A ==> B 子图 subgraph title graph definition end markdown latex [1] https://www.zybuluo.com/fyywy520/note/82980 "},"notes/vscode.html":{"url":"notes/vscode.html","title":"VSCode","keywords":"","body":"Visual Studio Code (VSCode) \r vscode & markdown markdown preview enhanced vscode & latex \r vscode & markdown [1] https://code.visualstudio.com/docs/languages/markdown markdown preview enhanced [1] https://shd101wyy.github.io/markdown-preview-enhanced/#/zh-cn/ [2] http://blog.csdn.net/m0_37639589/article/details/77684333 vscode & latex [1] https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop [2] http://www.jianshu.com/p/57f8d1e026f5 [3]https://tex.stackexchange.com/questions/353132/align-input-and-output-of-algorithm-to-left "},"notes/auxnet.html":{"url":"notes/auxnet.html","title":"Re-implementation of AuxNet","keywords":"","body":"Re-implementation of AuxNet Method Analysis Results on CIFAR "},"notes/git.html":{"url":"notes/git.html","title":"Git","keywords":"","body":"git ignore committed file [1] http://www.jianshu.com/p/e5b13480479b "}}