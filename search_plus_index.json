{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Welcome to iceory's blog~ "},"Network Quantization/Introduction.html":{"url":"Network Quantization/Introduction.html","title":"Network Quantization","keywords":"","body":"\r Introduction Deep convolutional neural networks (DCNN) have achieved great performance on computer vision tasks, including image classification, object detection and image segmentation. However, it is difficult to deploy deep learning (DL) methods in mobile devices, as deep neural networks always need high computational resource and large storage. Recent studies on adopting DL to hardware mainly focus on two folds: the first one is to design lighter architecture with comparable performance (i.e. SqueezeNet of Han et.al. ) ; the second fold is network quantization, which aims to compress models and speed up computation of DCNN by converting high precision data to low precision version. Other coding methods including Huffman Coding, Compressed Sparse Column (CSC) and Hashing Coding also help reducing size of models. Here, we try to compare several quantization and compression methods as shown in follows: Incremental network quantization (INQ), a lossless quantization method with incremental idea. Combined with dynamic network surgery (DNS), the authors compressed AlexNet for 53\\times without accuracy loss Binary weights network (BWN) and XNOR-Net Ternary weights network (TWN) Binarized neural networks (BNN) Trained ternary quantization (TTQ) Deep compression with network quantization, pruning and Huffman code "},"Network Quantization/Incremental Network Quantization.html":{"url":"Network Quantization/Incremental Network Quantization.html","title":"Incremental Network Quantization","keywords":"","body":"Quantization Methods \r INQ (Incremental Network Quantization) Weight Quantization Strategy Proof 1: compute factor 4/3 of $n_1$ Proof 2: compute $n_2$ Weight Partition Strategies Implementation in PyTorch Experimental Results \r INQ (Incremental Network Quantization) As most of the existing methods suffer from high decreasing on model performance and need many training epochs, the authors provided a lossless quantization method to overcome these problems. The proposed method mainly contains three steps: weight partition, group-wise quantization and re-training. Given a trained model, the first step of INQ is to divide weights of the model into to group, one for quantization and another for re-training. Second, apply weight quantization and convert 32-bits floating point data to low precision data. Third, freeze the quantized weights and retraining the network using SGD, then update remaining weights of the network. Repeating these three steps until all weights are quantized, then we can get a low precision model without significant accuracy loss. Considering binary shift operation is more efficient in hardware, the authors quantize weights of convolutional layers and fully connected layers to the power of 2. Weight Quantization Strategy Suppose weights of a pre-trained full precision model can be represented by \\{ W_l: 1 \\le l \\le L \\}, the quantized weights are represented by \\widehat{W_l} , where each item is chosen from P_l = \\{\\pm2^{n_1}, \\cdots, \\pm2^{n_2}, 0\\}. The quantization method is formulated by \\widehat{W_l}(i, j) = \\begin{cases} \\beta sgn(W_l (i, j))&if (\\alpha + \\beta)/2 \\le abs(W_l (i, j)) \\lt 3\\beta/2 &\\\\ 0 & otherwise, \\end{cases} \\tag{1} where \\alpha and \\beta are two adjacent elements in the sorted P_l. Based on Equation (1), n_1 and n_2 in P_l can be computed by n_1 = floor(log_2(4s/3)), where s=max(abs(W_l)); n_2 = n_1+1-2^{(b-1)}/2, where b is the quantized bit-width. Proof 1: compute factor 4/3 of $n_1$ Considering the extremely condition in Equation (1), we have (2^{n_1-1} + 2^{n_1})/2 \\le max(abs(W_l)) \\le3\\cdot2^{n_1}/2. Then, 2^{n_1-1}\\le2max(abs(W_l))/3\\lt2^{n_1}, n_1-1\\le log_2(2max(abs(W_l))/3)\\lt n_1 n_1\\le log_2(4max(abs(W_l))/3)\\lt n_1+1 Because floor(x)\\le x \\le ceil(x), then we let n_1=floor(4max(abs(W_l))/3). For simplifying the equation, define s=max(abs(W_l)), then we have n_1 = floor(log_2(4s/3)). Proof 2: compute $n_2$ As b denotes the expected bit-width, one bit for zero and others for representing the powers of 2, which including 2^{b-1} different values. Here we have (n_1-n_2+1)\\cdot2= 2^{b-1} according to definition of P_l. Thus n_2 can be computed by n_2 = n_1+1-2^{b-1}/2. Weight Partition Strategies In this paper, the authors explored two kinds of weight partition strategies, including random partition and pruning-inspired partition. The second partition strategy considers that weights with larger absolute values are more important than the smaller ones and would have more possibility to be quantized. The experimental results also shows that the pruning-inspired strategy outperforms the first one for about 0.8% with ResNet-18. Implementation in PyTorch Prepare: pre-train a full-precision model Step1, weight partition: decide number of weights to be quantized according to portion \\{\\sigma_1, \\sigma_2, \\cdots, \\sigma_n\\} generate quantization mask T_l(i, j)\\in \\{0,1\\} using pruning-inspired strategy Step2, group-wise quantization: quantize weights according to mask T_l in order to make sure the quantized weights not be changed during re-training phase, here we save weights as \\widehat{W} Step3, re-training: reset learning rate apply forward, backward and computing gradient update weights by using SGD reload quantized weight from \\widehat{W} partially according to mask T_l repeating operations 2 to 4 with full training phase. Repeat: repeat step1 to step 3 until all weights are quantized. Experimental Results The authors adopted the proposed method to several model, including AlexNet, VGG-16, GoogleNet, ResNet-18 and ResNet-50. More experiments for exploration was conducted on ResNet-18. Experimental results on ImageNet using center crop validation are shown as follows. Network Bit-width Top-1/Top-5 Error Decrease in Top-1/Top-5 Error Portion AlexNet ref 32 42.71%/19.77% AlexNet 5 42.61%/19.54% 0.15%/0.23% {0.3, 0.6, 0.8, 1.0} VGG-16 ref 32 31.46%/11.35% VGG-16 5 29.18%/9.70% 2.28%/1.65% {0.5, 0.75, 0.875, 1.0} GoogleNet ref 32 31.11%/10.97% GoogleNet 5 30.98%/10.72% 0.13%/0.25% {0.2, 0.4, 0.6, 0.8, 1.0} ResNet-18 ref 32 31.73%/11.31 ResNet 5 31.02%/10.90% 0.71%/0.41 {0.5, 0.75, 0.875, 1.0} ResNet-50 ref 32 26.78%/8.76% ResNet-50 5 25.19%/7.55% 1.59%/1.21% {0.5, 0.75, 0.875, 1.0} Number of required epochs for training increasing with the expected bit-width going down. The accumulated portions for weight quantization are set as {0.3, 0.5, 0.8, 0.9, 0.95, 1.0}, {0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0}, {0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.975, 1.0} for 4-bits to 2-bits, respectively. Training epochs required for 2-bits finally set to 30 which means that 300 training epochs are required for completing a full quantization procedure. In the other words, the proposed method become time-consuming when the network going deeper. Although the authors convert weights to the powers of 2 and claim that their method would be efficient with binary shift operation in hardware, the computation in there experiments is still using floating operations. Thus they only show the results of model compression instead of speeding up computation. "},"Network Quantization/Dynamic Network Surgery.html":{"url":"Network Quantization/Dynamic Network Surgery.html","title":"Dynamic Network Surgery","keywords":"","body":"DNS (Dynamic Network Surgery) \r Pruning and Splicing important tips Implementation in PyTorch Experimental Results \r In this paper, the authors proposed dynamic network surgery to prune unimportant connections of the network. Different from previous methods, the proposed method contains two operations: pruning and splicing. Considering the complexity of deep neural networks, it is difficult to decide which connection is important and which one should be pruned. Thus the splicing operation is to recover the pruned weights that are found to be important during training phase. In contrary, weights are pruned with no chance to come back in previous methods, and that may lead to severe loss of accuracy. Pruning and Splicing Suppose W_k represents weights of a DNN model, and mask T_k(i,j)\\in \\{0,1\\} denotes the states of connections of network. The optimize object can be formulated as follows: \\min_{W_k, T_k} L(W_k\\odot T_k)~s.t. T_k^{(i, j)}=h_k(W_k^{(i, j)}), \\forall (i,j)\\in \\mathcal{I}, \\tag{1} where L(\\cdot) denotes the loss function, \\odot indicates the Hadamard product operator), \\mathcal{I} represents every entry of weight matrixes and h_k(W_k^{(i,j)}) evaluates the importance of connections in the network. The definition of h_k is shown as follows: h_k(W_k^{(i,j)}) = \\begin{cases} 0 & if~a_k\\gt|W_k^{(i,j)}| \\\\ T_k^{(i,j)} & if~a_k\\le |W_k^{(i,j)}| \\lt b_k \\\\ 1 & if~b_k\\le |W_k^{(i,j)}| \\end{cases}\\tag{2} a_k and b_k in Equation (2) are two thresholds for improving the robustness of the proposed method. Note that if |W_k^{(i,j)}|\\in [a_k, b_k], the state of T_k^{(i,j)} would not be changed. This Equation is applied to the network to re-evaluate all the weights before training phase. Thus the pruned weights can be spliced if they are evaluated as important connections. Further more, the thresholds a_k and b_k are computed by the following equations according to source code on GitHub: a_k = 0.9*(mean(W_k)+cRate*std(W_k)),\\\\ b_k = 1.1*(mean(W_k)+cRate*std(W_k)), \\tag{3} where cRate is the factor deciding the compressing rate of the model. Larger cRate leads to higher compression rate with more loss of accuracy. After T_k is obtained by Equation (2), we can train the network with full training phase and update weights W_k with the following scheme: W_k^{i,j} \\gets W_k^{(i,j)}-\\beta \\frac{\\partial}{\\partial(W_k^{(i,j)}T_k^{(i,j)})}L(W_k\\odot T_k), \\forall(i,j)\\in \\mathcal{I} \\tag{4} Considering the convergence of the proposed method, the authors suggest that the frequencies of pruning and splicing can be slowed down with a monotonically non-increasing function \\sigma(\\cdot) which indicates the probability of mask T_k that to be modified in single training iteration. With amount of training iterations, the value of \\sigma may even decreased to zero, and the pruning and splicing to the network will be stopped. important tips The computation of thresholds only applied at the very beginning, and would not be change after that Considering the gradient vanishing problem, the authors prune convolutional layers and fully connected layers separately, which is similar to Han's paper Pruned weights would also be updated during training Implementation in PyTorch Prepare: pre-train a DNN model Step 1, pruning and splicing: compute a_k and b_k by Equation (3) and update mask T_k by Equation (2) with probability of \\sigma(iter) Step 2, training: adopt forward and backward propagation and compute Loss and Gradient Step 3, updating: update weights of the model with mask T_k using SGD Repeating: repeating step 1 to step 3 until the number of inter reaches its maximum Experimental Results The authors conducted experiments on several models including LeNet-5, LeNet-300-100 and AlexNet. The experimental results can be summarized as follows: Model Top-1 Error Parameters Iterations Compression LeNet-5 reference 0.91% 431K 10K LeNet-5 pruned 0.91% 4.0K 16K 108\\times LeNet-100-300 reference 2.28% 267K 10K LeNet-100-300 pruned 1.99% 4.8K 25K 56\\times AlexNet reference 43.42%/- 61M 450K AlexNet pruned 43.09%/19.99% 3.45M 700K 17.7\\times More detail comparison with work of Han et. al. on AlexNet using single crop validation on ImageNet are shown as follows: Layer Parameters Remaining Parameters Rate of Han et. al.(%) Remaining Parameters Rate(%) conv1 35K ~84% 53.8% conv2 307K ~38% 40.6% conv3 885K ~35% 29.0% conv4 664K ~37% 32.3% conv5 443K ~37% 32.5% fc1 38M ~9% 3.7% fc2 17M ~9% 6.6% fc3 4M ~25% 4.6% Total 61M ~11% 5.7% "},"Network Quantization/Binary Weight Networks.html":{"url":"Network Quantization/Binary Weight Networks.html","title":"Binary Weight Networks","keywords":"","body":"BWN (Binary Weight Networks) and XNOR-Net \r Binary Weight Networks Proof 1: solve objective function Implementation in PyTorch XNOR-Networks Binary Convolution Implementation in PyTorch Experimental Results \r Although deep neural networks have shown great potential in several application domains including computer vision and speech recognition, it is hard to implement DNN methods in hardware with the limitation of storage, compute capabilities and battery power. The authors in this paper proposed two efficient approximation to the neural network: binary weight networks (BWN) and XNOR-Networks. In binary weight networks, all the weights are approximated with binary values. While in XNOR-Networks, both the weights and the inputs to the convolutional layers and fully connected layers are approximated with binary values. The authors also attempted to evaluate their methods on large scale data sets like ImageNet, and proved that their methods outperform baseline for about 16.3%. Source code is available on GitHub. Binary Weight Networks Represent an L-layer DNN model with a triplet . Each element I=\\mathcal{I}_{l(l=1,\\cdots,L)} in \\mathcal{I} is the input tensor of the l^{th} layer, W=\\mathcal{W}_{lk(k=1,\\cdots, K^l)} is the k^{th} weight filter in the l^{th} layer of DNN. * represents convolutional operation with I and W. Note that the authors assume the convolutional layers in the network do not have bias terms. Thus the convolutional operation can be approximated by I*W\\approx(I\\oplus B)\\alpha, where \\oplus indicates a convolution without multiplication, B=\\mathcal{B}_{lk} is a binary filter \\alpha=\\mathcal{A}_{lk} is an scale factor and \\mathcal{W}\\approx\\mathcal{A}_{lk}\\mathcal{B}_{lk}. The optimized objective function is shown as follows: J(B,\\alpha) = \\| W-\\alpha B \\|^2 \\\\ \\alpha^*, \\beta^* = \\arg\\min_{\\alpha, B} J(B,\\alpha) \\tag{1} By solving the objective function mentioned above, we get: B^* = sign(W) \\\\ \\alpha^* = \\frac{1}{n} \\|W\\|_{l1} \\tag{2} Proof 1: solve objective function J(B,\\alpha) = \\| W-\\alpha B \\|^2 = \\alpha^2B^\\intercal B - 2\\alpha W^\\intercal B + W^\\intercal W \\tag{3} Since B\\in\\{+1, -1\\}^n, B^\\intercal B=n are constants, and W^\\intercal W is also constant as W is a known value. Thus the Equation (3) can be re-written as: J(B,\\alpha) =\\alpha^2n-2\\alpha W^\\intercal B+c \\tag{4} Note that \\alpha is a positive value in Equation (4), then the solution of B^* is: B^* = \\arg\\min_B\\{W^\\intercal B\\}~~s.t. B\\in\\{+1, -1\\}^n \\\\ B* = sign(W) \\tag{5} Take the derivative of J, we have: \\frac{\\partial J}{\\partial\\alpha} = 2\\alpha n-2W^\\intercal B = 0 \\\\ \\alpha^* = \\frac{W^\\intercal B^*}{n} = \\frac{W^\\intercal sign(W)}{n} = \\frac{\\sum |W_i|}{n} = \\frac{1}{n}\\|W\\|_{l1} \\tag{6} Implementation in PyTorch Prepare: pre-train a DNN model (the authors did not mention that they use a pre-trained model or training from scratch) Step 1, quantization: quantize weights of convolutional layers using Equation (2) Step 2, training: apply standard forward and backward propagation to the network Step 3, update parameters: update parameters with standard SGD Repeat: repeat step 1 to step 3 until reach max iterations Note that weights in BWN are still floating-point as the author use \\alpha B to approximate W. \\alpha is the mean of weights in every convolutional layer. The main idea of the authors is computing the best approximation of weights. However, optimal approximation to weights do not means optimal approximation to the final outputs of the network. Thus this kinds of methods may still lead to high loss of accuracy. XNOR-Networks Based on the proposed binary weight networks, the authors further explore the method to binarize both weights and inputs. Convolutional operation consist of shift operation and dot product, and if the dot product can be expressed by binary operations, then convolution can be approximated using binary operations. The approximation of dot product between X, W\\in \\mathbb{R}^n can be expressed by X^\\intercal W\\approx \\beta H^\\intercal \\alpha B, where H,B\\in \\{+1,-1\\}^n and \\beta, \\alpha\\in \\mathbb{R}^+, then the optimized objective function is: \\alpha^*,B^*, \\beta^*, H^* = \\arg\\min_{\\alpha, B,\\beta, H} \\| X\\odot W - \\beta\\alpha H\\odot B\\|, \\tag{7} where \\odot indicates the element-wise product. Define Y\\in\\mathbb{R}^n such that Y_i = X_i W_i,~C\\in\\{+1,-1\\}^n such that C_i=H_i B_i and \\gamma\\in\\mathbb{R}^n such that \\gamma=\\beta\\alpha. Equation (7) can be written as: \\gamma^*, C^* = \\arg\\min_{\\gamma,C}\\|Y-\\gamma C\\| \\tag{8} Similar to the Equation (6), the optimal solutions of Equation (8) are shown as follows: C^* = sign(Y) = sign(X)\\odot sign(W) = H^* \\odot B^* \\\\ \\gamma^* = \\frac{\\sum{|Y_i|}}{n}=\\frac{\\sum{|X_i||W_i|}}{n} \\approx(\\frac{1}{n}\\|X\\|_{l1})(\\frac{1}{n}\\|W\\|_{l1}) = \\beta^*\\alpha^* \\\\ H^* = sign(X) \\\\ B^* = sign(W) \\\\ \\beta^*= \\frac{1}{n}\\|X\\|_{l1} \\\\ \\alpha^*= \\frac{1}{n}\\|W\\|_{l1} \\tag{9} Binary Convolution With the binary weights and binary inputs of convolutional layer, the convolution operations can be approximated using binary operations: I*W \\approx (sign(I)\\circledast sign(W)) \\odot K\\alpha \\tag{10} where \\circledast indicates a convolutional operation using XNOR and bit-count operations, K contains scale factors \\beta for all sub-tensors in input I. The authors suggested that applying pooling on binary input results in significant loss of information, thus they changed the standard convolutional blocks to pre-activated version. Binary Gradient: similar to the binarization in the forward pass, the gradient in the backward pass can also be binarized. To preserve the maximum change of gradient, the authors use \\max_i(|g_i^{in}|) as the scale factor. k-bit Quantization: the authors also mentioned that 1-bit quantization of weights can also be converted to k-bit quantization by using q_k(x)=2(\\frac{[(2^k-1)(\\frac{x+1}{2})]}{2^k-1}-\\frac{1}{2} instead of sign(x) function, where [\\cdot] indicates rounding operation and x\\in [-1,1]. Implementation in PyTorch The implementation of XNOR-Net is similar to those of BWN which including three steps: quantization, forward propagation and backward propagation. Experimental Results In this paper, the authors conducted experiments on AlexNet and ResNet and validated the proposed methods using validation data set of ImageNet with single crop. The optimization method used in there experiments is ADAM as it can converge faster and have better performance with binary inputs. Experimental results are shown as follows: Model Top-1 Accuracy/ Top-5 Accuracy Accuracy Loss AlexNet reference 56.6% / 80.2% AlexNet BC 35.4% / 61.0% -21.2% / -19.2% AlexNet BWN 56.8% / 79.4% 0.2% / -0.8% AlexNet BNN 27.9 % / 50.42% -28.7% / -29.78% AlexNet XNOR-Net 44.2% / 69.2% -12.4% / -11% ResNet-18 reference 69.3% / 89.2% ResNet-18 BWN 60.8% / 83.0% -8.5% / -5.8% ResNet-18 XNOR-Net 51.2% / 73.2% -18.1% / -16% GoogLeNet reference 71.3% / 90.0% GoogLeNet BWN 65.5% / 86.1% -5.8% / -3.9% "},"Network Quantization/Ternary Weight Networks.html":{"url":"Network Quantization/Ternary Weight Networks.html","title":"Ternary Weight Networks","keywords":"","body":"TWN (Ternary Weight Networks) \r Training Methods Important Tips Experimental Results \r The authors introduced ternary weight networks (TWNs) to address the limited storage and computational resources issues in hardware. The quantization problem can be formulated as follows: \\begin{cases} \\alpha^*, W^{t*} = &\\arg\\min_{\\alpha, W^t} J(\\alpha, W^t) = \\|W-\\alpha W^t\\|_2^2 \\\\ s.t. & a\\ge0, W_i^t\\in\\{-1,0,1\\}, i=1,2,\\dots, n. \\end{cases} \\tag{1} Here n is the size of filter, W represents weights of the network. With W\\approx \\alpha W^t and assuming the convolutional layer do not have bias term, forward propagation of ternary weight networks is as follows: \\begin{cases} Z & = &X*W \\approx X*(\\alpha W^t) = (\\alpha X)\\oplus W^t \\\\ X^{next} & = & g(Z) \\end{cases} \\tag{2} where X indicates inputs, * indicates convolutional operation, g is the non-linear activation function, \\oplus indicates the inner product or convolutional operation without any multiplication, X^{next} indicates the outputs. The approximated solution of W with threshold-based ternary function is as follows: W_i^t = f_t(W_i|\\triangle) = \\begin{cases} +1, if~W_i \\gt \\triangle \\\\ 0, if~|W_i| \\le \\triangle \\\\ -1, if~W_i \\lt -\\triangle \\end{cases} \\tag{3} The optimized objective function can be written as: \\alpha^*, \\triangle^* = \\arg\\min_{\\alpha\\ge0, \\triangle\\gt 0}(|I_\\triangle|\\alpha^2-2(\\sum_{i\\in I_\\triangle}|W_i|)\\alpha+c_\\triangle) \\tag{4} where I_\\triangle = \\{i|~|W|\\gt\\triangle\\} and |I_\\triangle| denotes the number of elements in I_\\triangle; c_\\triangle = \\sum_{i\\in I_\\triangle^c}W_i^2 is a \\alpha-independent constant. Thus the optimal solutions of the objective function can be computed as follows: \\alpha_\\triangle^* = \\frac{1}{|I_\\triangle|}\\sum_{i\\in I_\\triangle}|W_i| \\\\ \\triangle^* = \\arg\\max_{\\triangle\\gt 0}(\\sum_{i\\in I_\\triangle}|W_i|^2) \\tag{5} Here solution of \\triangle is approximated by \\triangle^*\\approx0.7\\cdot E(|W|) \\approx \\frac{0.7}{n}\\sum_{i=1}^n|W_i|. Training Methods The training of ternary weight networks can be summarized to three steps: quantization, training and updating. Quantization phase is to quantize the weights of convolutional layers using Equation (5), then apply standard forward and backward propagation to the network, and update parameters using standard SGD. Source code is available on GitHub. Important Tips \\alpha is used as the scaling factor for input X not for weights W gradient is computed using W^t quantized weights are used during forward and backward but not during parameters update, so W_l^r \\gets W_l^r - \\eta \\frac{\\partial C}{\\partial W_l^t} first compute \\triangle then compute mask, finally compute \\alpha \\triangle is computed with all |W^r| while \\alpha is computed only with those |W^r|>\\triangle apply weight_decay would lead results worse this blog is useful for implementation Experimental Results Three data sets are used in this paper, including MNIST, CIFAR-10, ImageNet. To different data sets, the authors conducted experiments using LeNet-5 (32-C5 + MP2 + 64-C5 + MP2 + 512FC + SVM), VGG-inspired network (2\\times(128-C3) + MP2 + 2\\times(256-C3) + MP2 + 2\\times(512-C3) + MP2 + 1024-FC + Softmax), ResNet-18, respectively. Network architecture and parameters setting for different data sets are shown as follows: MNIST CIFAR-10 ImageNet network architecture LeNet-5 VGG-7 ResNet-18 (B) weight decay 1e-4 1e-4 1e-4 mini-batch size of BN 50 100 64(\\times4 GPUs) initial learning rate 0.01 0.1 0.1 learning rate decay (divided by 10) epochs 15, 25 80, 120 30, 40, 50 momentum 0.9 0.9 0.9 Comparison of the proposed method and the previous methods are shown as follows: Method MINIST CIFAR-10 ImageNet Top1 (ResNet-18 / ResNet-18B) ImageNet Top5 (ResNet-18 / ResNet-18B) TWN 99.35 92.56 61.8 / 65.3 84.2 / 86.2 BPWN 99.05 90.18 57.5 / 61.6 81.2 / 83.9 FPWN (full precision) 99.41 92.88 65.4 / 67.6 86.76 / 88.0 Binary Connect 98.82 91.73 - - Binarized Neural Networks 88.6 89.85 - - Binary Weight Networks - - 60.8 83.0 XNOR-Net - - 51.2 73.2 "},"Network Quantization/Trained Ternary Quantization.html":{"url":"Network Quantization/Trained Ternary Quantization.html","title":"Trained Ternary Quantization","keywords":"","body":"TTQ (Trained Ternary Quantization) The ternary quantization methods proposed in this paper based on threshold and quantized weights to 0 and {-1, +1} with two different scaling factor. The authors also suggested the rule of scaled gradient to update weight in different group. Quantized ternary weight w_l^i of the network can be calculated by: w_l^t = \\begin{cases} W_l^p&:& \\tilde{w_l} \\gt \\triangle_l \\\\ 0&:& |\\tilde{w_l}| \\le \\triangle_l \\\\ -W_l^n&:& \\tilde{w_l} \\lt -\\triangle_l \\end{cases} \\tag{1} Different from previous work where weights are calculated from 32-bit weights, the scaling coefficients W_l^p, W_l^n are two independent parameters and are trained with gradient descent: \\frac{\\partial L}{\\partial W_l^p} = \\sum_{i\\in I_l^p}\\frac{\\partial L}{\\partial w_l^t(i)}, \\\\ \\frac{\\partial L}{\\partial W_l^n} = \\sum_{i\\in I_l^n}\\frac{\\partial L}{\\partial w_l^t(i)}, \\tag{2} Here I_l^p=\\{i|\\tilde{w_l}(i) \\gt \\triangle_l\\} and I_l^n=\\{i|\\tilde{w_l}(i) \\lt -\\triangle_l\\}. The proposed scaled gradients for 32-bit are weights computed by: \\frac{\\partial L}{\\partial\\tilde{w_l}} = \\begin{cases} W_l^p\\times\\frac{\\partial L}{\\partial w_l^t}&:&\\tilde{w} \\gt \\triangle_l \\\\ 1\\times\\frac{\\partial L}{\\partial w_l^t}&:&|\\tilde{w}| \\le \\triangle_l \\\\ W_l^n\\times\\frac{\\partial L}{\\partial w_l^t}&:&\\tilde{w} \\lt -\\triangle_l \\end{cases} \\tag{3} Different from TWN, the authors explored two strategies to decide the values of thresholds: (1) \\triangle_l=t\\times\\max(|\\tilde{w}|), t is set to 0.05 on CIFAR-10 and ImageNet; (2) \\triangle_l=r, where r is a hyper-parameter and adjusted with various sparsities. For exploring the trade-off between sparsity and accuracy, the sparsity of weights are growing from 0 to 0.5 and the best result occurs with sparsity between 0.3 and 0.5. Training Methods First: normalize full-precision weights to range [-1, +1] by dividing each weight by the maximum weight Second: quantize weights to \\{-W_l^n, 0, W_l^p\\} with threshold factor t Third: compute the scaled gradients and update the scaling coefficients with back propagation Source code is available on GitHub. Experimental results Testing error on CIFAR-10 with ResNet is shown as follows: Model Full precision Trained Ternary Quantization Improvement ResNet-20 8.23 8.87 -0.64 ResNet-32 7.67 7.63 0.04 ResNet-44 7.18 7.02 0.16 ResNet-56 6.80 6.44 0.36 The authors further evaluate their method on ImageNet with AlexNet and ResNet-18B. To AlexNet, the authors preserves full precision weights in first convolutional layer and the last fully-connected layer, and other layer parameters are quantized to ternary values. Experimental results are shown as follows: Model Bit-width ImageNet Top-1/Top-5 Error Accuracy Loss AlexNet reference 32 42.8 / 19.7 - AlexNet DoReFa 1 46.1 / 23.7 -3.3 / -4.0 AlexNet TWN 2 45.5 / 23.2 -2.7 / -3.5 AlexNet TTQ 2 42.5 / 20.3 -0.3 / -0.6 ResNet-18B reference 32 30.4 / 10.8 - ResNet-18B DoReFa 1 39.2 / 17.0 -8.8 / -6.2 ResNet-18B TWN 2 34.7 / 13.8 -4.3 / -3.0 ResNet-18B TTQ 2 33.4 / 12.8 -3.0 / -2.0 "},"Network Quantization/Binarized Neural Networks.html":{"url":"Network Quantization/Binarized Neural Networks.html","title":"Binarized Neural Networks","keywords":"","body":"BNN (Binarized Neural Networks) \r Proposed Method Binarization Strategies Gradient Shift-based Batch Normalization Shift-based AdaMax Binarized Input Training Method Experimental Results Extension of BNN Experimental Results \r In this paper, the authors proposed a method to train Binarized Neural Networks (BNNs), a network with binary weights and activations. The proposed BNNs drastically reduce the memory consumption (size and number of accesses) and have higher power-efficiency as it replaces most arithmetic operations with bit-wise operations. The code implemented in Theano and Torch is available on GitHub. Proposed Method Binarization Strategies Constrain both weights and activation to either +1 or -1 has higher efficiency in hardware. The authors discussed two binarization functions including deterministic and stochastic. Formulation of deterministic binarization function is: x^b = sign(x)= \\begin{cases} +1 & if ~x\\ge 0 \\\\ -1 & otherwise, \\end{cases} \\tag{1} The stochastic binarization function is: x^b = \\begin{cases} +1, & \\mathrm{with~probability}~p=\\sigma(x) \\\\ -1, & \\mathrm{with~probability}~1-p, \\end{cases} \\tag{2} where \\sigma is the \"hard sigmoid\" function: \\sigma(x) = clip(\\frac{x+1}{2},0,1) = \\max(0,\\min(1,\\frac{x+1}{2})) \\tag{3} The authors suggested that the stochastic binarization is harder to implement as it requires the hardware to generate random bits, though it is more appealing than the deterministic binarization, so they preferred to use the deterministic binarization function in their experiments. Gradient Real-valued gradients are computed and accumulated in real-valued variables in this paper, as high precision is required for SGD. Previous work shows that using \"straight-through estimator\" can help the network training faster, the authors used straight-through estimator of \\frac{\\partial C}{\\partial r} simplified as: g_r = g_q1_{|r|\\le1} \\tag{4} which cancels the gradient when r is too large. The derivation 1_{|r|\\le1} can also be seen as propagating the gradient through hard tanh: \\mathrm{Htanh}(x)=clip(x,-1,1)=\\max(-1,\\min(1,x)) \\tag{5} The real-valued weights w^r first projected to [-1,+1] and then quantized to binarized weights w^b using w^b=sign(w^r). Shift-based Batch Normalization The authors proposed a shift-based batch normalization (SBN) to achieve the results of BN so as to speed up computation of batch normalization. The algorithm is shown as follows: \\mu_B \\gets \\frac{1}{m}\\sum_{i=1}^m x_i \\\\ C(x_i) \\gets (x_i-\\mu_B) \\\\ \\sigma^2_B \\gets \\frac{1}{m} \\sum_{i=1}^m (C(x_i)\\ll\\gg AP2(C(x_i))) \\\\ \\hat{x_i} \\gets C(x_i) \\ll \\gg AP2((\\sqrt{\\sigma^2_B+\\epsilon})^{-1}) \\\\ y_i \\gets AP2(\\gamma) \\ll \\gg \\hat{x_i} \\tag{6} Where AP2 is the approximate power-of-2, \\ll\\gg indicates both left and right binary shift operations. Shift-based AdaMax Since ADAM requires many multiplications, the authors suggested to use shift-based AdaMax which is shown as follows: m_t \\gets \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\ v_t \\gets \\max(\\beta_2 \\cdot v_{t-1}, g|t|) \\\\ \\theta_t \\gets \\theta_{t-1} - (\\alpha \\ll \\gg (1-\\beta_1)) \\cdot \\hat{m} \\ll \\gg v_t^{-1} \\tag{7} Where g_t^2 indicates the element-wise square g_t\\circ g_t. Good default setting are \\alpha=2^{-10},1-\\beta_1=2^{-3},1-\\beta_2=2^{-10}. All operations on vectors are element-wise and \\beta_1^t, \\beta_2^t denote \\beta_1 and \\beta_2 to the power t. Binarized Input Since the input representation has much fewer channels than the internal representations in computer vision and it is easy to convert continuous-valued inputs to fixed point numbers, the authors suggested to compute output of first layer by: s=x \\cdot w^b \\\\ s=\\sum_{n=1}^8 2^{n-1}(x^n \\cdot w^b) \\tag{8} where x is a vector of 1024 8-bit inputs, x_1^8 is the most significant bit of the first input, w^b is a vector of 1024 1-bit weights and s is the resulting weighted sum. Training Method Step 1, forward: binarized weights and apply SBN Step 2, backward: compute real-valued gradient g_a with constraint descripted in Equation (4), and compute gradient of weights Step 3, update: update weights with constraint descripted in Equation (4) Repeating: repeating step 1 to step 3, until finish the training. Experimental Results The authors evaluated their method on three data sets including MNIST, SVHN and CIFAR-10, results are shown as follows: Method MNIST SVHN CIFAR-10 BNN Torch7 1.40% 2.53% 10.15% BNN Theano 0.96% 2.80% 11.40% Extension of BNN Following the work of BNN, the authors proposed a training method to improve performance of BNN in four folds: (1) using low learning rate (the authors suggested to use the learning rate of 1e-4); (2) using PReLU instead of ReLU to absorb the scaling factor for weights to the activation function; (3) introducing a regularization term to the loss function to encourage the weights to be bipolar; (4) using scale layer in fully connected layer to bring the outputs to normal. The regularization term introduced in this paper is formulated by: J(W,b) = L(W,b)+\\lambda \\sum_{l=1}^L \\sum_{i=1}^{N_l} \\sum_{j=1}^{M_l} (1-(W_{l,ij})^2) \\tag{9} To improve the accuracy, the authors used multiple binarizations for the activation: A_l \\approx \\sum_{i=1}^m (\\alpha_{l,i} H{l,i}) \\tag{10} For i=1, H_{l,1} is the sign of A_l and \\alpha_{l,i} is the average absolute value of A_l, for i\\gt 1, H_{l,i} and \\alpha_{l,i} is calculated in the way based on residual approximation error from step i-1: E_{L,I} = a_l-\\sum_{j=1}^{i-1}\\alpha_{l,j}\\ast H_{l,j}. So the output O_l is calculated by: O_l = W_l \\cdot A_{l-1} \\approx \\sum_{i=1}^m (\\alpha_{l-1,i}xnor-popcnt(B_l, H_{l-1,i})) \\tag{11} Experimental Results The authors conducted experiments on ImageNet with AlexNet and NIN, the results are shown as follows: Method Bits of Activation Precision of Last Layer Compression Rate Accuracy AlexNet BNN 1 Full 10.3\\times 50.4/ 27.9 AlexNet XNOR-net 1 Full 10.3\\times 69.2 / 44.2 AlexNet DoReFa 2 Full 10.3\\times - / 49.8 AlexNet Extended-BNN 2 Binary 31.2\\times 71.1 / 46.6 NIN Extended-BNN 2 Binary 23.6 \\times 75.6 / 51.4 "},"Network Quantization/Deep Compression.html":{"url":"Network Quantization/Deep Compression.html","title":"Deep Compression","keywords":"","body":"Deep Compression In this paper the authors introduces \"deep compression\" to compress model size of deep convolutional neural networks. The proposed method consists of three stage: pruning, trained quantization and Huffman coding. The authors first prunes weights by learning the important connections. Second, quantize weights to enforce weight sharing. Third, apply Huffman coding to reduce storage further. Proposed Methods Network Pruning: First, learn connectivity by normal network training. Second, remove weights below threshold. Third, retrain the network to learn the final weights for remaining sparse connections. After pruning, the sparse structure is stored using compress sparse row (CSR) or compress sparse column (CSC). Trained Quantization: use k-clusters with b-bits to represent n-connections of the network. The compression rate can be computed by: r=\\frac{nb}{nlog_2(k)+kb} \\tag{1} The authors use k-means algorithm to get k-clusters C=\\{c_1, c_2,\\dots,c_k\\} to represent n original weights W=\\{w_1, w_2, \\dots,w_n\\}, and the optimized objective function is: \\arg\\min_C\\sum_{i=1}^{k}\\sum_{w\\in c_i} |w-c_i|^2 \\tag{2} As the initialization of k-means algorithm is quite important, the authors explored three initial methods including forgy (random), density-based, linear. The authors suggested that using linear initialization can get better result as it has better representation to the few large weights which are important to the networks. Once the centroids of weights are decided by k-means clustering, the index of sharing weight table is stored for each connections and used when conducting forward or backward propagation. Gradients for the shared weights are computed by: \\frac{\\partial \\mathcal{L}}{\\partial C_k} = \\sum_{i,j}\\frac{\\partial \\mathcal{L}}{\\partial W_{i,j}}\\frac{\\partial W_{i,j}}{\\partial C_k} = \\sum_{i,j}\\frac{\\partial{\\mathcal L}}{\\partial W_{i,j}}\\mathbb{1}(I_{i,j}=k), \\tag{3} where \\mathcal{L} indicates loss, W_{i,j} indicates weight in the i-th column and j-th row, C_k indicates the k-th centroid of the layer and \\mathbb{1}(\\cdot) is an indicator function. Experimental Results The authors conducted experiments on two data sets: on MNIST, they used LeNet-300-100 and LeNet-5 while used AlexNet and VGG-16 on ImageNet to evaluate the proposed methods. Results on MINIST are summarized as follows: Model Top-1 (/ Top-5 Error) Accuracy Loss Parameters Compression Rate LeNet-300-100 reference 1.64% - 1070 KB - LeNet-300-100 compressed 1.58% 0.06% 27 KB 40 \\times LeNet-5 reference 0.80% - 1720 KB - LeNet-5 compressed 0.74% 0.06% 44 KB 39 \\times AlexNet reference 42.78% / 19.73% - 240 MB - AlexNet compressed 42.78% / 19.70% 0% / 0.03% 6.9 MB 35 \\times VGG-16 reference 31.50% / 11.32% - 552 MB - VGG-16 compressed 31.17% / 10.91% 0.33% / 0.41% 11.3 MB 49 \\times "},"Network Quantization/Integer Arithmetic Only Inference.html":{"url":"Network Quantization/Integer Arithmetic Only Inference.html","title":"Integer Arithmetic Only Inference","keywords":"","body":"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference \r Training with simulated quantization Point-wise quantization: compute gradient Inference with integer-arithmetic only Data type Affine mapping from q to r The Following operations Batch normalization folding Graph illustration simple graph for single layer layer with bypass convolutional layer with batch normalization \r Paper: https://arxiv.org/abs/1712.05877 Code: refer to TensorFlowLite.quantize Training with simulated quantization Point-wise quantization: clamp(r;a,b) := min(max(r, a), b) s(a,b,n) := \\frac{b-a}{n-1} q(r;a,b,n):=\\lfloor \\frac{clamp(r;a,b)-a}{s(a,b,n)} \\rceil s(a,b,n)+a Here, $r$ represents the real value, $q$ represents the quantized value. $\\lfloor \\cdot \\rceil$ denotes rounding to the nearest integer. for weights: $a:=min(w), b:=max(w)$ for activation: collect [a;b] during training and aggregate them via exponential moving average (EMA) exponential moving average (EMA) in tensorflow: shallow_variable -= (1-decay) * (shallow_variable-variable) reasonable values for decay are close to 1.0, e.g., 0.999, 0.99999, etc Note: activation quantization is disabled at the start of training compute gradient \\frac{\\partial{L}}{\\partial{r}} = \\frac{\\partial{L}}{\\partial{q}}\\frac{\\partial{q}}{\\partial{r}} Here, we have $\\frac{\\partial{q}}{\\partial{r}}=0$ if $r\\notin[a,b]$, otherwise $\\frac{\\partial{q}}{\\partial{r}}=1$. Inference with integer-arithmetic only Data type input: uint8 weights: uint8 bias: int32 activation: int32 output: uint8 Affine mapping from q to r Formulation: r = S(q-Z) \\to q=\\frac{r}{S}+Z where $S$ means \"Scale\" and $Z$ means \"Zero point\". And $S=s(a,b,n), Z=z(a,b,n)$. Therefore, considering $r_3=r_1*r_2$: r_3 = S_3(q_3-Z_3), r_1*r_2=S_1S_2(q_1-Z_1)(q_2-Z_2) q_3 = \\frac{S_1S_2}{S_3}(q_1-Z_1)(q_2-Z_2)+Z_3 Let $M:=\\frac{S_1S_2}{S_3}$ and $M=2^{-n}M_0$, $M_0\\in(0.5,1]$. For matrix multiplication of two matrices with size of $N\\times N$. q_3^{(i,k)} = Z_3 +M\\sum_{j=1}^{N}(q_1^{(i,j)}-Z_1)(q_2^{(j,k)}-Z_2) It needs $O(N^3)$ subtraction to compute the result. More efficient implementation: q_3^{(i,k)} = Z_3 + M(\\sum_{j=1}^N q_1^{(i,j)}q_2^{(j,k)}-\\sum_{j=1}^N q_1^{(i,j)}Z_2-\\sum_{j=1}^N q_2^{(j,k)}Z_1 + \\sum_{j=1}^N Z_1Z_2) q_3^{(i,k)} = Z_3 + M(\\sum_{j=1}^N q_1^{(i,j)}q_2^{(j,k)}-Z_2 \\bar a_1^{(i)}- Z_1a_2^{(k)} + N Z_1Z_2), where $\\bar a1^{(i)}:=\\sum{j=1}^N q1^{(i,j)}$ and $a_2^{(k)}:=\\sum{j=1}^N q2^{(j,k)}$. Therefore, the computational costs is mainly from the computation of $\\sum{j=1}^N q_1^{(i,j)}q_2^{(j,k)}$ The Following operations scale down: int32 activation --> int8 output activation cast down: int8 activation --> uint8 output Batch normalization folding w_{fold}:=\\frac{\\gamma w}{\\sqrt{EMA(\\sigma_B^2)+\\epsilon}} Graph illustration simple graph for single layer origin quantized layer with bypass origin quantized convolutional layer with batch normalization training inference training with fold training with fold quantized "},"notes.html":{"url":"notes.html","title":"笔记","keywords":"","body":""},"notes/gitbook.html":{"url":"notes/gitbook.html","title":"Install and Use Gitbook on Windows","keywords":"","body":"gitbook 安装及使用（windows）参考 \r 安装node.js 使用npm安装gitbook gitbook使用 常用命令 构建github blog gitbook 插件 支持中文搜索的search-pro gitbook editor 如何切换预览模式？ \r 安装node.js 直接到官网下载exe安装即可，参考网站 使用npm安装gitbook 注意：不知道什么原因，gitbook命令只能在nodejs的终端中使用，在系统的cmd或者powershell中不被识别 npm install gitbook-cli -g 在node.js command prompt 中使用 gitbook -V 确认安装成功 gitbook使用 打开node.js command prompt 运行gitbook 常用命令 本地预览： gitbook serve ./book_name 输出静态网站： gitbook build ./book_path ./output_path 查看帮助： gitbook help 构建github blog 注意： 仓库名称必须为usename.github.io，注意大小写，跟官方教程有点不一样，这个后期再弄。参考 创建github远程仓库 克隆仓库到本地： git clone .... 创建一个新的分支 git checkout -b gh-pages(分支名必须为gh-pages) 将分支push到仓库 切换到主分支master 将gitbook build得到的静态网页文件复制到改仓库的本地目录下 提交到远程仓库 打开http://username.github.io可以看到网页版本的gitbook gitbook 插件 支持中文搜索的search-pro github: https://github.com/gitbook-plugins/gitbook-plugin-search-pro 在终端运行命令： npm install gitbook-plugin-search-pro 修改book.json: { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ] } gitbook editor 如何切换预览模式？ 新版的gitbook editor将预览模式的切换收入了右下角的“？”那里，选择“Edit Markdown”即可切换。 参考链接： https://www.zhihu.com/question/56703698/answer/150102664 "},"notes/manual_vs_auto_gradient.html":{"url":"notes/manual_vs_auto_gradient.html","title":"Softmax Loss with Different Differentiation Methods","keywords":"","body":"Manual Gradient V.S. Auto Gradient (PyTorch) \r Reproduction of Softmax Loss with Cross Entropy softmax function Cross entropy loss Coding in PyTorch Using basic function of PyTorch testing code: Reference: \r github: https://github.com/ICEORY/softmax_loss_gradient.git Reproduction of Softmax Loss with Cross Entropy softmax function the softmax function is defined by \r y_i = \\frac{e^{x_i}}{\\sum e^{x_k}}, for~i=1,..., C\r where $x$ is the input with $C$ channels, $y$ is the respected output. the gradient of softmax $\\frac{\\partial y_i}{\\partial x_j}$ is computed by: \r {\\rm if}~i=j,~\\frac{\\partial y_i}{\\partial x_j}=\\frac{\\partial y_i}{\\partial x_i} = \\frac{e^{x_j}\\cdot \\sum e^{x_k}-e^{x_i}\\cdot e^{x_i}}{(\\sum e^{x_k})^2} = \\frac{e^{x_i}}{\\sum e^{x_k}}\\frac{\\sum e^{x_k}-e^{x_i}}{\\sum e^{x_k}} = y_i \\cdot (1-y_i)\r \r {\\rm if}~i\\ne j,~\\frac{\\partial y_i}{\\partial x_j}=\\frac{\\partial \\frac{e^{x_j}}{\\sum e^k}}{\\partial x_j} = \\frac{0\\cdot \\sum e^{x_k}-e^{x_i}\\cdot e^{x_j}}{(\\sum e^{x_k})^2} = -\\frac{e^{x_i}}{\\sum e^{x_k}} \\frac{e^{x_j}}{\\sum e^{x_k}} = -y_i \\cdot y_j\r Cross entropy loss the cross entropy loss function is \r \\mathcal L = -\\sum_{c=1}^C t_c\\cdot log(y_c),\r where $t$ is the one-hot label. For a batch of samples, the cross-entropy loss can be re-written to \r \\mathcal L = -\\sum_{n=1}^N \\sum_{c=1}^C t_{nc} \\cdot log(y_{nc})\r the gradient of cross entropy loss is computed by \r \\frac{\\partial \\mathcal L}{\\partial x_i} = -\\sum_{c=1}^C \\frac{\\partial t_c log (y_c)}{\\partial x_i} = -\\sum_{c=1}^C t_c \\frac{\\partial log(y_c)}{\\partial x_i} = -\\sum_{c=1}^C t_c\\frac{1}{y_c}\\frac{\\partial y_c}{\\partial x_i} = -\\frac{t_i}{y_i}\\frac{\\partial {y_i}}{\\partial x_i}-\\sum_{i\\ne j}^C \\frac{t_j}{y_j}\\frac{\\partial y_j}{\\partial x_i} = -\\frac{t_i}{y_i}y_i(1-y_i) - \\sum_{i\\ne j}^C \\frac{\\partial t_j}{y_j}(-y_i y_j) = -t_i+t_i y_i + \\sum_{i\\ne j}^C t_j y_i = -t_i +y_i\\sum_i^C t_i = y_i-t_i\r Coding in PyTorch Using basic function of PyTorch forward propagation of SoftMaxLoss def forward(self, x, target): \"\"\" forward propagation \"\"\" assert x.dim() == 2, \"dimension of input should be 2\" exp_x = torch.exp(x) y = exp_x / exp_x.sum(1).unsqueeze(1).expand_as(exp_x) # parameter \"target\" is a LongTensor and denotes the labels of classes, here we need to convert it into one hot vectors t = torch.zeros(y.size()).type(y.type()) for n in range(t.size(0)): t[n][target[n]] = 1 output = (-t * torch.log(y)).sum() / y.size(0) # output should be a tensor, but the output of sum() is float output = torch.Tensor([output]).type(y.type()) self.y = y # save for backward self.t = t # save for backward return output to use the auto-grad scheme of PyTorch, we also define a function to execute the same operation of forward propagation of softmax loss def SoftmaxLossFunc(x, target): exp_x = torch.exp(x) y = exp_x / exp_x.sum(1).unsqueeze(1).expand_as(exp_x) t = torch.zeros(y.size()).type(y.data.type()) for n in range(t.size(0)): t[n][target.data[n]] = 1 t = Variable(t) output = (-t * torch.log(y)).sum() / y.size(0) return output backward propagation: def backward(self, grad_output): \"\"\" backward propagation \"\"\" grad_input = grad_output * (self.y - self.t) / self.y.size(0) return grad_input, None testing code: def test_softmax_loss_backward(): \"\"\" analyse the difference between autograd and manual grad \"\"\" # generate random testing data x_size = 3200 x = torch.randn(x_size, x_size) # .cuda() # use .cuda for GPU mode x_var = Variable(x, requires_grad=True) # convert tensor into Variable # testing labels target = torch.LongTensor(range(x_size)) target_var = Variable(target) # compute outputs of softmax loss y = SoftmaxLoss()(x_var, target_var) # clone testing data x_copy = x.clone() x_var_copy = Variable(x_copy, requires_grad=True) # compute output of softmax loss y_hat = SoftmaxLossFunc(x_var_copy, target_var) # compute gradient of input data with two different method y.backward() # manual gradient y_hat.backward() # auto gradient # compute difference of gradients grad_dist = (x_var.grad - x_var_copy.grad).data.abs().sum() outputs: the distance between our implementation and PyTorch auto-gradient is about e-7 under 32 bits floating point precision, and our backward operation is slightly faster than the baseline ===================================================== |===> testing softmax loss forward distance between y_hat and y: 0.0 |===> testing softmax loss backward y: Variable containing: 8.5553 [torch.FloatTensor of size 1] y_hat: Variable containing: 8.5553 [torch.FloatTensor of size 1] x_grad: Variable containing: -3.1247e-04 1.3911e-07 4.8041e-07 ... 3.0512e-08 1.7696e-08 1.0826e-07 7.6744e-07 -3.1246e-04 1.2172e-07 ... 1.2465e-07 6.0764e-08 5.0740e-08 8.7925e-08 1.7995e-08 -3.1242e-04 ... 1.1499e-07 6.7635e-08 5.2739e-08 ... ⋱ ... 1.0118e-08 1.7118e-07 1.7081e-07 ... -3.1244e-04 3.1381e-07 2.1709e-08 2.2232e-07 2.4775e-07 1.0417e-07 ... 4.6105e-08 -3.1172e-04 2.1110e-08 1.6006e-07 4.8581e-08 3.2675e-08 ... 2.3572e-07 5.3878e-08 -3.1247e-04 [torch.FloatTensor of size 3200x3200] x_copy.grad: Variable containing: -3.1247e-04 1.3911e-07 4.8041e-07 ... 3.0512e-08 1.7696e-08 1.0826e-07 7.6744e-07 -3.1246e-04 1.2172e-07 ... 1.2465e-07 6.0764e-08 5.0740e-08 8.7925e-08 1.7995e-08 -3.1242e-04 ... 1.1499e-07 6.7635e-08 5.2739e-08 ... ⋱ ... 1.0118e-08 1.7118e-07 1.7081e-07 ... -3.1244e-04 3.1381e-07 2.1709e-08 2.2232e-07 2.4775e-07 1.0417e-07 ... 4.6105e-08 -3.1172e-04 2.1110e-08 1.6006e-07 4.8581e-08 3.2675e-08 ... 2.3572e-07 5.3878e-08 -3.1247e-04 [torch.FloatTensor of size 3200x3200] distance between x.grad and x_copy.grad: 1.11203504294e-07 |===> comparing time-costing time of manual gradient: 1.13225889206 time of auto gradient: 1.40407109261 with 64 bits double precision, the difference of gradient is reduced into e-16. Notice that the outputs of two sofmaxloss function have a gap of e-7. Again, our method is slightly faster. ===================================================== |===> testing softmax loss forward distance between y_hat and y: 2.31496107617e-07 |===> testing softmax loss backward y: Variable containing: 8.5468 [torch.DoubleTensor of size 1] y_hat: Variable containing: 8.5468 [torch.DoubleTensor of size 1] x_grad: Variable containing: -3.1246e-04 4.7302e-08 2.6106e-08 ... 2.1885e-08 1.5024e-08 6.0311e-09 4.1688e-08 -3.1245e-04 1.1503e-07 ... 1.8215e-07 3.1857e-08 1.1914e-07 9.2476e-08 7.1073e-08 -3.1248e-04 ... 2.7795e-08 2.5479e-07 4.8765e-08 ... ⋱ ... 5.0167e-08 1.2661e-07 8.0579e-08 ... -3.1239e-04 2.0139e-08 1.3870e-08 2.8047e-07 3.2061e-07 1.8310e-08 ... 1.5054e-08 -3.1248e-04 8.4565e-08 5.4617e-08 4.3503e-08 5.2926e-08 ... 1.2573e-07 3.3953e-08 -3.1236e-04 [torch.DoubleTensor of size 3200x3200] x_copy.grad: Variable containing: -3.1246e-04 4.7302e-08 2.6106e-08 ... 2.1885e-08 1.5024e-08 6.0311e-09 4.1688e-08 -3.1245e-04 1.1503e-07 ... 1.8215e-07 3.1857e-08 1.1914e-07 9.2476e-08 7.1073e-08 -3.1248e-04 ... 2.7795e-08 2.5479e-07 4.8765e-08 ... ⋱ ... 5.0167e-08 1.2661e-07 8.0579e-08 ... -3.1239e-04 2.0139e-08 1.3870e-08 2.8047e-07 3.2061e-07 1.8310e-08 ... 1.5054e-08 -3.1248e-04 8.4565e-08 5.4617e-08 4.3503e-08 5.2926e-08 ... 1.2573e-07 3.3953e-08 -3.1236e-04 [torch.DoubleTensor of size 3200x3200] distance between x.grad and x_copy.grad: 1.99762357071e-16 |===> comparing time-costing time of manual gradient: 1.170181036 time of auto gradient: 2.39760398865 Reference: [1] http://shuokay.com/2016/07/20/softmax-loss/ [2] https://en.wikipedia.org/wiki/Cross_entropy "},"notes/pytorch_autograd.html":{"url":"notes/pytorch_autograd.html","title":"PyTorch and Automatic Differentiation","keywords":"","body":"Automatic Differentiation of PyTorch \r 自动求导概述 自动求导 PyTorch 自动求导 Reference: \r 自动求导概述 参考文献： Automatic Differentiation in Machine Learning: a Survey 目前用于求导的方法大致可以分为四类： 手动解析并代码实现 数值求导 (numerical differentiation), 使用有限的微分近似 符号求导 (symbolic differentiation), 使用代数表达式进行计算 自动求导 (automatic differentiation, also called algorithmic differentiation) 自动求导与数值求导、符号求导两者都不同，如果没有详细的了解的话，容易将自动求导归为数值求导。事实上，自动求导只提供一个数值的结果，并不能像符号求导一样提供一个完整的表达式，但它也确实是根据符号求导的规则进行，通过反向追踪表达式的运算过程实现求导。因此，自动求导兼具了数值求导与符号求导两者的特性。 数值求导的表达式如下： \r \\frac{\\partial f({\\bold x})}{\\partial x_i} \\approx \\frac{f(x-h {\\bold e}_i)-f({\\bold x})}{h},\r 其中 ${\\bold e}_i$ 是第 $i$ 单位向量，$h \\gt 0$ 是一个小的步长。 符号求导主要根据一系列的求导规则进行变换，例如 \r \\frac{d}{dx}(f(x)+g(x)) \\leadsto \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)\r 或 \r \\frac{d}{dx}(f(x)g(x)) \\leadsto (\\frac{d}{dx}f(x))g(x)+f(x)(\\frac{d}{dx}g(x))\r 从优化的角度，符号求导可以给出问题的内部结构，并用于结果分析。然而，符号求导也容易得到冗长的符号表达式，导致计算困难。 自动求导 自动求导依赖于所有的数值计算都是由有限的元操作构成，并且这些元操作的求导是已知的。依据链式法则，可以将所有组成操作的求导过程联系起来，完成整体的求导。自动求导的模式包括：前向模式以及反向模式。 以 $f(x_1,x_2)={\\rm ln}(x_1)+x_1x_2-{\\rm sin}(x_2)$ 的求导为例子。其计算过程可以由下图表示。 前向模式求解过程： 反向模式求解过程： PyTorch 自动求导 在PyTorch中的自动求导是tape-based autograd，换句话说是基于类似反向模式的自动求导。通过动态的构建运算图，然后反向传播对各个成分进行求导。在PyTorch中，基本的元操作的求导过程已经写好在程序中，如： class Sinh(Function): @staticmethod def forward(ctx, i): ctx.save_for_backward(i) return i.sinh() @staticmethod def backward(ctx, grad_output): i, = ctx.saved_variables return grad_output * i.cosh() 其中Function是一个重要的类，所有元操作或者需要自定义求导过程的操作都从Function类继承，可以记录操作的轨迹并用于自动求导的过程。 PyTorch动态图构建过程如下图所示： Reference: [1] https://justindomke.wordpress.com/2009/03/24/a-simple-explanation-of-reverse-mode-automatic-differentiation/ [2] https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/ [3] python autograd tool: https://github.com/HIPS/autograd [4] https://github.com/pytorch/pytorch/tree/v0.2.0 "},"notes/caffe2.html":{"url":"notes/caffe2.html","title":"Install Caffe2 on Windows","keywords":"","body":"Caffe2 reference [1] https://caffe2.ai/docs/getting-started.html?platform=windows&configuration=compile [2] http://research.wmz.ninja/articles/2017/05/build-caffe2-on-windows-10-gpu.html "},"notes/markdown.html":{"url":"notes/markdown.html","title":"Markdown Tutorial","keywords":"","body":"markdown \r markdown 基本教程 markdown mermaid 其他基本语法： 形状 连接方式 markdown latex \r markdown 基本教程 参考 [1]：http://itmyhome.com/markdown/article/extension/task-list.html 主要包括 markdown 的基本语法、扩展语法、编辑器以及扩展、格式转换等方面的内容； markdown mermaid 参考 [1] https://mermaidjs.github.io/flowchart.html mermaid 是 markdown 中用于画图的工具，基本语法如下： graph TD start-->stop graph LR start-->stop 其中 graph TD 表示画图的方向，表示方向的语法： TB - top bottom BT - bottom top RL - right left LR - left right TD - same as TB 其他基本语法： 形状 带文字的节点 graph LR id1[this is a node] 带圆角的节点 graph LR id1(this is a node with round edges) 圆形的节点 graph LR id1((this is a circle)) 不对称形状：id1>This is the text in the box] 斜方形：id1{This is the text in the box} 连接方式 实线箭头： A-->B 实线连接： A --- B 带文字连接： A-- This is the text ---B 或者 A---|This is the text|B 带文字箭头连接： A-->|text|B A-- text -->B 虚线连接： A-.->B; 粗箭头： A ==> B 子图 subgraph title graph definition end markdown latex [1] https://www.zybuluo.com/fyywy520/note/82980 "},"notes/vscode.html":{"url":"notes/vscode.html","title":"VSCode","keywords":"","body":"Visual Studio Code (VSCode) \r vscode & markdown markdown preview enhanced vscode & latex \r vscode & markdown [1] https://code.visualstudio.com/docs/languages/markdown markdown preview enhanced [1] https://shd101wyy.github.io/markdown-preview-enhanced/#/zh-cn/ [2] http://blog.csdn.net/m0_37639589/article/details/77684333 vscode & latex [1] https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop [2] http://www.jianshu.com/p/57f8d1e026f5 [3] https://tex.stackexchange.com/questions/353132/align-input-and-output-of-algorithm-to-left "},"notes/auxnet.html":{"url":"notes/auxnet.html","title":"Re-implementation of AuxNet","keywords":"","body":"Re-implementation of AuxNet Method Analysis Results on CIFAR "},"notes/git.html":{"url":"notes/git.html","title":"Git","keywords":"","body":"git ignore committed file [1] http://www.jianshu.com/p/e5b13480479b "},"notes/powerpoint.html":{"url":"notes/powerpoint.html","title":"Power Point","keywords":"","body":"PPT使用经验 \r PPT可以做什么？ 插件 关于PPT美化大师中广告的问题 画图规范 常用快捷键 简易教程 基本操作 组合图形 曲线绘制 例子参考 \r PPT可以做什么？ 展示：海报、汇报等 美工：制作插图（矢量图） 修图：制作表情包等 插件 以下的插件都可以在网络上搜索获取，并很方便的插入到PPT里面使用。 PPT美化大师：这个用到的最多。主要使用对齐工具，不过会有广告，如有更好的插件请推荐。 OneKey Tools：一键统一图的大小、特效等。（偶尔用上） iSlides：一键优化、PPT拼图等。（基本用不上） 关于PPT美化大师中广告的问题 答：在安装目录下寻找 minisite.exe 以及 notify.exe，编辑里面的内容使得这两个程序无效，或者新建一个txt文件，重命名为\"minisite.exe\"/\"notify.exe\"，替换掉这两个文件。然后设置文件的权限为只读（不然又会被恢复）。 画图规范 不要嫌麻烦，尽量做好每个细节； 多练习；参考论文里面画的不错的图来练习； 想清楚图片要表示什么信息； 注意对齐。居中、左对齐、右对齐、等距分布等；可以在视图里面设置网格线方便对齐。 线宽一般为1.5~2.0宽度。根据实际需要会有不同的变化； 配色采用同种颜色的不同深浅搭配。 常用快捷键 复制：选择对象，CTRL+鼠标拖动 全选：CTRL+a 格式刷： 复制格式：CTRL+shift+c 粘贴格式：CTRL+shift+v 取消自动对齐：鼠标操作的基础上，按住alt 按比例缩放图形：鼠标操作的基础上，按住CTRL 按中心缩放图形：鼠标操作的基础上，CTRL+shift ---------------------内容分割线---------------------------------- 简易教程 基本操作 绘制基本的图形，选择对象并按住ctrl拖动复制 改变线框为1.5，修改线条颜色以及填充颜色 效果图： 快速改变形状：选择对象，在格式里面选择“编辑对象”，然后选择目标图形 效果图： 组合图形 目标图形 图形分析 绘制基本图形 合并图形操作： 选择对象（两个或者以上），在菜单栏“格式”里面选择“合并图形”，选择“拆分” 联合两个半圆还有一个矩形，得到一个圆角矩形（虽然系统已经提供了这个形状，不过是以此为例子而已） 使用矩形对圆角矩形进行分割 继续使用上述操作，得到最终的目标 曲线绘制 目标图形（接下来以此为参考对象绘制图形） 使用曲线工具进行轮廓绘制，注意这里要绘制成封闭的图形 选中曲线所绘制的轮廓，右键，选择“编辑顶点” 继续调整，得到最终图形 例子参考 "},"notes/pyplot.html":{"url":"notes/pyplot.html","title":"PyPlot","keywords":"","body":"PyPlot 绘图笔记 改变字体类型 reference: http://blog.csdn.net/ginynu/article/details/70808962 import matplotlib.pyplot as plt plt.rc('font',family='Times New Roman') 自动调整边界范围 reference: https://stackoverflow.com/questions/4042192/reduce-left-and-right-margins-in-matplotlib-plot plt.tight_layout() "},"fun.html":{"url":"fun.html","title":"作品展示","keywords":"","body":""},"Some Thing Fun/HelloworldGame.html":{"url":"Some Thing Fun/HelloworldGame.html","title":"HelloWorldGame","keywords":"","body":"Hello World Game GitHub地址：https://github.com/ICEORY/HelloWorldGame 功能说明 一个简单的吃豆豆游戏，吃完豆豆看看还能不能到达终点。 结果展示 "},"Some Thing Fun/基于CPLD的电梯仿真系统.html":{"url":"Some Thing Fun/基于CPLD的电梯仿真系统.html","title":"基于CPLD的电梯仿真系统","keywords":"","body":"五层楼的简易电梯控制系统 \r 整体方案设计 电路设计及原理说明 按键电路设计 keyinput模块设计 updownkey模块设计 floorchoice模块设计 时钟电路设计 10fenpin 模块设计 timescale模块设计 系统决策电路设计 ENCODE_LE模块设计 nextfloor模块设计 电机电路设计 motion模块设计 doormotor模块设计 楼层计数电路设计 100counter模块设计 5counter模块设计 floorruncount模块设计 排气、照明、蜂鸣器电路设计 otherdevice模块设计 数码管显示电路设计 display模块设计 整机电路设计以及原理说明 整机电路设计 原理说明 性能测试及其结果 上下行按键测试 情景模拟测试 遇到的问题以及解决方案 模块化编程的问题 仿真时间的设置 输出端口的合并 关于Timing 跟Functional 仿真的区别 关于out of block的解决方案 关于按键初始状态的说明 附录 设计文件中模块名称与对应的功能说明 PCB制板电路原理图 整体电路原理 时钟电路 电机电路 按键输入 数码管显示 楼层指示及蜂鸣器 PCB布线图 样板测试 \r GitHub地址：https://github.com/ICEORY/ElevatorCPLD 整体方案设计 项目本着“各个击破，各司其职”的指导思想进行设计。各个击破是指将整个电梯控制系统分为各个小模块进行设计，在完成小模块设计之后再进行组合最终实现需要的功能；各司其职指在设计的时候保证每个小模块执行自己要实现的功能，能够正确处理设计范围内的输入。 将整个控制系统根据功能细分为各个小功能模块，主要利用JK触发器74113o以及8-3编码器74148、数值比较器7485等数字模块搭建出系统需要的各个小模块，最后根据各个模块的之间的逻辑联系构建出整个系统。 在使用QuartusII软件搭建各个模块时，每搭建一个功能模块就要对其输入输出逻辑进行仿真，保证其输入输出的波形图满足需要的逻辑功能。当每个模块都能各司其职时，最终搭建整个控制系统才能做到事半功倍，清晰有条理。 根据题目的设计要求，整个电梯控制系统结构图如下所示： 根据上述功能，将电路分为以下几个模块： 模块名称 目标实现功能 keyinput 获取单个按键输入，并能够存储按键状态，能够根据外部输入重置按键状态 doormotor 轿厢门电机，控制开关门动作，进行10ms计时，9s工作周期，其中开关门各一秒，停止7s，具有复位重新计时功能 motion 20s工作周期，能够根据上下行方向控制三相输出端的输出顺序 timescale 输入1KHZ时钟信号，进行分频之后输出1HZ,10HZ,100HZ,1KHZ的时钟信号 floorcount 输入楼层到达信号时上升沿触发，根据运行方向进行所存储的楼层数的加减，并输出计算结果 nextfloor 根据运行方向上的按键信息判断是否需要继续运行，判断是否需要开门，判断下次运行方向 otherdevice 对电梯其他辅助模块如蜂鸣器、排气、照明等设备进行控制 display 轿厢内、电梯门口的电梯所在楼层信息显示，包括所在楼层，运行方向 电梯运行的逻辑如下所示： 电路设计及原理说明 按键电路设计 keyinput模块设计 根据功能要求，该模块的应具备以下的输入输出端口： input output KEY KEYSTATE DONE DO CLK 在逻辑上应满足： 接收到外界按键信息时，将输出的KEYSTATE强制为1； 当接收到复位指令DONE=1时，将KEYSTATE复位为0，并输入DO信号，用于指示其他模块执行任务；如：用于开门信号； 根据以上要求，使用JK触发器搭建该子模块,真值表如下： KEYSTATE真值表： KEY DONE KEYSTATE KEYSTATE* 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 DO真值表： DONE KEYSTATE DO DO* 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 根据计算得到的逻辑关系使用74113搭建出如下电路： 仿真结果如下所示： updownkey模块设计 构建完单个按键的输入电路之后利用该模块进一步构建电梯上下行选择按键输入；该模块由由八个keyinput单元构成；其输入输出如下表所示： input output UP1~UP4 KEYSTATE1~KEYSTATE8 DOWN2~DOWN5 DOOR`~DOOR5 CLK FLOORA~FLOORC U/D ARRIVE 根据五层电梯的实际情况，一楼只有上行按钮，五楼只有下行按钮，中间三层具有上下行按钮，共八个按键输入，输出控制五层楼的轿厢外门电机，故具有五个门控制输出，八个按键状态输出。 使用74138三-八译码器将输入的所在楼层信息译成八个按键状态的控制信息，其逻辑真值表如下： FLOORC FLOORB FLOORA Y0 Y1 Y2 Y3 Y4 Y5 Y6 Y7 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 各个按键与输入的ARRIVE、U/D逻辑关系满足：（以Y2为例） ARRIVE U/D Y2 DONE 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 该模块的输出DOOR逻辑上等于各个按键子模块输出的DO 电路如下所示： 该模块整体仿真波形如下： 图中以第三层的上下行按键为例，在电梯没有到达三层时按下上下行按键，可见电梯不会进行开关门动作，并且将按键状态存储。在电梯到达三层时，由于电梯运行方向为上行（U/D=1），此时只判断上行方向的按键（KEYSTATE4），进行开门动作并将按键状态复位。电梯在三层的时候若运行方向改变为下行（U/D=0），此时进行开门动作，但由于同时在三层再次按下下行键故按键的按下状态一直持续到按键松开为止而，开门动作一直持续到楼层发生变化。可见，该模块设计符合实际电梯运行的逻辑。 floorchoice模块设计 电梯轿厢内的楼层选择仍选择用上述的按键模块，电路逻辑与updownkey模块的设计类似，其中keyinput模块满足： KEYDOWN=KEY； DONE=Y’ARRIVE’。 Floorchoice模块同时控制轿厢门电机的运动： DOOR=DO1+D2+DO3+DO4+DO5; 电路设计如下所示： 该模块整体仿真波形如下： 图中以一层与二层的按键选择为例，在电梯到达之前，根据输入将按键状态进行保存。在电梯到达相应楼层时进行开门动作并将对应按键复位。 时钟电路设计 10fenpin 模块设计 由于该电梯输入时钟频率为10kHz，故设计一个十分频电路将输入的时钟进行十分频，用于输出1Hz,10Hz,100Hz,1kHz等频率的时钟用于各个模块工作需要。使用74190十进制计数器作为分频核心模块，其逻辑真值表如下： QD QC QB QA CO 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 电路设计如下所示： timescale模块设计 使用上述10fenpin模块构建多频率输出的timescale模块用于提供各个模块的时钟信号。模块原理同异步计数器的设计原理，电路图如下所示： 系统决策电路设计 ENCODE_LE模块设计 由于8-三编码器74148是高位优先编码器，根据电梯上下楼层排序的需要，在下行时需要对输入进行低位优先编码，故以编码器74148为核心设计一个五-三低位优先编码器；设计原理是先将输入进行倒序处理,模块输入与74148芯片接口如下： IN4 0N IN3 1N IN2 2N IN1 3N IN0 4N 根据上表可以对输入的低位进行优先编码，但由此导致了输出与输入的错位，使用门电路对输出进行校正，使输入IN0=1时，输出编码A2=0;A1=0;A3=0;真值表如下所示： A2N A1N A0N A2 A1 A0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 此外当外界没有输入时，A2=1，A1=1;A0=1; 电路设计如下所示： 仿真波形如下图所示： nextfloor模块设计 使用上述构建的ENCODE_LF模块与高位优先编码器74148搭建电梯的核心决策模块nextfloor，该模块根据按键的输入以及当前电梯所在的楼层输出电梯即将进行的动作，输入输出列表如下： KEYSTATE1~KEYSTATE8(上下行按键选择) OPEN（开门信号） CHOICE1~CHOICE5（目标楼层选择） RUN（电梯运行信号） FLOORA~FLOORC（当前电梯的位置） DIR（电梯运行方向） CLK 使用高位优先编码器对输入的KEYSTATE以及CHOICE进行高位优先编码，输出其中的按键的最大值并使用数值比较器7485将键值与当前电梯所处的楼层进行比较决定电梯是否继续运行，设U=1表示电梯上行方向上有按键按下，且键值大于当前楼层；U=0 表示电梯上行方向没有键值大于当前楼层的按键按下；设D=1 表示电梯下行方向有键值小于当前楼层的按键按下；D=0表示电梯下行方向没有键值小于当前楼层的按键按下；电梯是否继续运行的判断值RUN应该满足RUN=U+D; 而电梯运行的方向DIR与U/D关系的真值表如下： U D DIR DIR* 0 0 0 X 0 0 1 X 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 当U/D均为0时需要对电梯的运行方向进行进一步的判断，U/D为零一般出现在第一次按下上下行按键时出现，此时电梯位置与按键所在楼层一致且没有其他按键按下，故设UE=1表示电梯所在位置有上行方向的按键按下且没有其他按键选择；DE=1表示电梯所在位置有下行方向的按键按下且没有其他按键选择； 此时RUN=0,DIR与UE/DE的关系如下： UE DE DIR DIR* 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 开门信号OPEN=RUN’DE 电路设计如下图所示： 电机电路设计 motion模块设计 由于本课题所使用的轿厢电机设定为四线三相步进电机，故需要根据输入的运行方向以及开门信号输出相应的三相电压的变化，电机正转时运行逻辑为A-B-C-A,反转时逻辑为A-C-B-A。使用双JK触发器74113构建三进制触发器，其计数顺序为00-01-10-00。 Q2*Q1*的卡诺图如下所示： Q2 Q1 0 1 0 01 10 1 00 XX ​ 电机控制输出的逻辑真值表如下所示： RUN U/D Q2Q1 A B C 0 X X 0 0 0 1 0 00 1 0 0 1 0 01 0 1 0 1 0 11 0 0 1 1 1 00 1 0 0 1 1 01 0 0 1 1 1 10 0 1 0 此外根据题目要求，每两个楼层之间电机运行时间为20s，故使用十进制计数器74190与二进制计数器74161构成二十进制计数器。 电路设计如下所示： doormotor模块设计 在电梯到达目标楼层之后，电梯需要控制门电机依次执行以下动作：开门1s—停止7s—关门1s；其中在关门1s时间内根据电梯实际运行方向判断相应的按键输入，如上行时上行按键按下或者下行时下行按键按下则门重新打开，或者在门关闭时遇到障碍物也重新打开门。由分析可知，以上三种门重新打开的情况只需要归结为一个复位开门的指令给门电机控制模块即可。 由于本课题的门电机选择直流电机，故只需要输出MOTOR1/MOTOR2控制电机的正反转。使用三个74161十六进制计数器搭建900进制计数器，输入100Hz时钟信号控制。在0s时启动MOTOR1，关闭MOTOR2，此时直流电机正转，在1s时关闭MOTOR1，同时保持MOTOR2关闭，此时直流电机不转，在8s时保持MOTOR1关闭，启动MOTOR2，此时直流电机反转，9s时关闭MOTOR2，同时输出电机关门完成信号。** 电路逻辑真值表如下： RUN Q9~Q0 MOTOR2 MOTOR1 DONE 0 XXXXXXXXXX 00 1 1 0000000000 01 0 1 0001100100 00 0 1 1100100000 10 0 1 1110000100 00 1 电机运行复位，即计数清零端CLRN’真值表如下： RUN MOTOR2 RESET CLRN 0 X X 0 1 0 X 1 1 1 0 1 1 1 1 0 则电路设计如下图所示： 楼层计数电路设计 100counter模块设计 根据题目要求需要对各个楼层进行运行次数计数，考虑到运行次数可能比较多，故采用100进制计数模块，该模块是由两个74190十进制计数器构成的异步计数器，电路原理图如下所示： 5counter模块设计 楼层计数时为了节省数码管显示模块的数量，故增加对计数显示的选择按键，每按下一次按键显示的目标楼层自加1，从而实现用同一个对不同楼层计数的显示。由于该课题针对5层楼的电梯设计，故计数选择也是5进制的。为了节省芯片内部模块的资源，故设计该5进制模块用于计数选择。 电路设计如下： 仿真波形如下： floorruncount模块设计 使用上述100进制计数器搭建每个楼层的运行次数计数模块，输入输出信号列表如下： input output FLOORA~FLOORC FHLED-楼层计数十位显示 DONE FLLED-楼层计数个位显示 KEYINPUT CLED-按键选择楼层显示 通过三八译码器将输入楼层信号译成五个楼层的选择信号，当DONE=1时该楼层计数增加1；设计原理updownkey模块类似。电路原理如下所示： 排气、照明、蜂鸣器电路设计 otherdevice模块设计 将题目要求的其他附属硬件要求整合在该模块中。该模块主要实现对电梯的照明排气、蜂鸣器的控制。根据题目要求可知蜂鸣器在电梯到达目标楼层时响起提示，在本设计中设置为响0.5s，故使用三个JK触发器构建满足以下真值表条件的计数器。 DONE Q2Q1Q0 Q2Q1Q0* BEE 1 XXX 000 0 0 000 001 1 0 001 010 1 0 010 011 1 0 100 101 1 0 101 110 1 0 110 110 0 BEE为蜂鸣器的总控制标志，与输入的FLOORA~FLOORC楼层信号配合决定各个楼层蜂鸣器的状态。 电路设计如下所示： 数码管显示电路设计 display模块设计 根据题目要求，设计该模块用于轿厢内部以及电梯门外部的楼层信息显示，显示模块由一位数码管以及两个方向显示数码管构成，考虑到各个楼层的显示信息都是一致的，故只需要设计同一个信号输出端口即可。此外考虑到在设计中000表示第一层，100表示第五层，从人性化角度上应该做出如下转换： FLOORC~FLOORA LEDC~LEDA 000 0001 001 010 010 011 011 100 100 101 电梯运行方向的显示真值表如下： RUN DIR UP DOWN 0 X 0 0 1 0 0 1 1 1 1 0 电路设计如下所示： 整机电路设计以及原理说明 整机电路设计 根据第一节的系统设计框图将各个模块根据设计逻辑连接到一起，整体电路如下图所示： 原理说明 由于每个模块都各司其职，在第二节的设计中经过功能仿真保证了模块的功能实现。将各个模块的输入输出口对应相连即可实现第二章需要的功能。其中时钟模块输入为10KHz，输入各个频率信号到各模块，楼层计数模块输入信号为电梯轿厢电机运行结束的跳变信号，输出跳变之后的楼层信号到其他各个需要楼层信号的模块。此外核心决策模块，nextfloor主要根据按键的情况输出决策：RUN/DIR/OPEN 控制门电机以及轿厢电机的运行。因此将这些输出端口作为相关模块的输入。 性能测试及其结果 上下行按键测试 使用1KHz 的时钟信号，设置如下输入信号： 根据实际电梯运行的逻辑，由于首先按下的是上行按钮，故电梯在一楼会先开门，然后由于在开门过程中一次接收到来自DOWN2、DOWN3、UP2、DOWN4、DOWN5、UP3、UP4 的上下行按键信号，由于这些按键按下都是在电梯处于一楼开门过程发生的，因此电梯会对这些信息按照上下行的方向进行排序。一楼为上行优先，故排序结果为UP2、UP3、UP4、DOWN5、DOWN4、DOWN3、DOWN2，这意味着电梯会一次从一楼运行到五楼，再从五楼运行到二楼停止，在这过程中每一层楼都会开门一次，而且每次开门都会有一个提示音响起。 在电梯运行结束之后，保持运行方向的数据不变，电梯内的排气以及照明关闭。逻辑运行仿真图如下所示： 可见整个电梯运行逻辑符合现实运行的逻辑顺序。此外由于电梯在整个过程中在二楼开门次数为2 次，故根据楼层计数显示，二楼的计数次数为2次。而在楼层次数显示的选择按钮按下之前，系统显示的是1 楼的数据，开门之后一楼的运行次数为1.选择按钮按下之后，显示跳变为二楼的次数，此时为0 次； 情景模拟测试 初始状态下电梯停靠在一楼，并且电梯门关闭。此时在2 楼有人按下上行按键打算去五楼，而在他按下之后在4 楼有人按下下行按键打算去一楼，模拟输入如下所示： 根据实际的运行逻辑，电梯在一楼不开门直接上行到二楼并开门，进入到电梯之后按下五楼的选择按键，在这过程中恰好四楼有人按下下行按键。电梯开始上行到五楼，进行开门动作，之后电梯下行到四楼开门，人进去之后按下一楼运行按键，电梯运行到一楼之后开门并停止。在楼层计数方面，先不输入选择信号，此时显示的是1 楼运行次数。根据设计原理，1楼的运行次数在电梯在一楼开门之后为由0 变为1。此后保持不变。 仿真结果如下，在该输入信号之下系统可以实现符合现实逻辑的输出。 遇到的问题以及解决方案 模块化编程的问题 由于刚接接触QuartusII 软件的设计，对模块化设计还是比较陌生的。但由于之前用c 语言编程的时候习惯用h 文件对各个子程序进行模块设计故在开始本次电梯设计的时候便碰到如何使用该软件进行模块设计的问题。 在查阅了相关资料之后总结以下几点： 设计流程：先建立BDF 文件进行电路设计，然后点击FILE 里面的Create Symbol File 即可生成该模块； 各个模块需要功能明确，模块名称以功能命名； 每个模块都有两个文件，包括bdf 以及bsf两个文件，这样两个文件的名称一定要一致，并且在设计完成之后不可删除bdf 文件，否则模块会出错； 仿真时间的设置 一般来说系统的仿真时间默认为1ms，所以在需要进行比较长的仿真就需要对系统的仿真时间进行修改。通过查阅资料可知具体操作为：点击EDIT菜单选择EndTime，将时间修改为需要的时间即可。 输出端口的合并 在仿真的时候，如果具有很多的输出端口，便会对同步观察波形产生一定的困难。如果有一些端口，如表示楼层的三个端口，则可以将其合并显 示为10 进制的数。 操作为：选择需要合并的端口，点击右键Group，并命名即可。这样会大大缩减端口量，也便于观察。 关于Timing 跟Functional 仿真的区别 在仿真中一般选择Functional 进行仿真，有时错点为Timing 仿真之后波形图会变得出乎意料。主要原因是，Functional 仿真只是针对于功能的仿真，对时间要求不高。而Timing 仿真则是充分考虑了竞争冒险以及线延时等时间上的问题，故仿真的结果比较接近现实，更容易发现竞争冒险的问题。 关于out of block的解决方案 由于所使用的模块为CPM240T系列的芯片，该芯片限制用户最多只能使用240个block，在设计的时候通过compile的filter中的report可以查询实际使用的资源情况，包括每个模块包含的block信息等。在本次电梯设计中，由于项目比较复杂，故到后来即使出现了一些bug也不能通过编程解决。出现的问题包括：在选择楼层计数模块中原来按照100计数的模块由于该out of block的原因退而选择了40进制计数，每个子模块减少2个JK触发器。此外，该电梯在设计本身存在一个bug，也就是在关门1s内按下按键时可以恢复开门状态。但在设计的时候统一设计为按下任意门按键即可开门，无论是哪个楼层。该问题的解决方案是通过把一个或门改为相应的多个与或门即可，但由于block的限制，未能付诸实施。在本次的电梯设计中，通过多次对模块的简化，最终使用block数量为238个，使用率达99%。 关于降低block的使用率有以下几个方法： 降低IO口使用数量，由于QuartuesII本身具有很强的优化功能，故降低IO口的的使用数量可以有效的简化模块，使block使用率大幅度降低。例如原来使用200以上个block，但删除一个没必要的IO端口之后可以降低到100左右； 降低JK触发器的使用，因为每个JK触发器在计算上是一个block，因此从理论上降低一个触发器也就减少了一个block，这也是个人将计数从100进制降低到40进制的原因。 减少一些与或非门等门电路的使用，这个方法不一定凑效。因为系统本身优化的原因，有一些门电路实际上已经是被优化过了，存不存在其实没多大关系。但有一些没被简化过的，通过个人的简化可以降低block的使用。 关于按键初始状态的说明 初始设计为按键默认为低电平状态，在按下的时候出现高电平，但是由于PCB设计的原因后来改为初始状态为高电平，按下时低电平有效。PCB设计原来考虑CPLD芯片的引脚在初始状态应该为低电平，但实际为需要接入上拉电阻才能作为按键输入。否则一直保持低电平状态，并不会变为高电平。但由于设计的错误，最终只能外接上拉电阻，导致按键初始状态为高电平，按下时由于引脚接地而变为高电平。因此，在硬件固定之后只能通过改软件的设计来适应。故在本文中提及的功能不变而各个模块会出现一些出入。特此说明。 附录 设计文件中模块名称与对应的功能说明 模块名称 对应功能 备注 10fenpin.bsf 10分频电路 100counter.bsf 100进制计数器 display.bsf 轿厢以及电梯门口数码管显示 doormotor.bsf 门电机控制模块 ENCODE_LF.bsf 低位优先8-3编码器 floorchoice.bsf 轿厢内楼层选择按键输入 keyinput.bsf 按键输入模块 motion.bsf 电梯轿厢电机控制模块 otherdevice.bsf 蜂鸣器、排气、照明电路控制模块 timescale.bsf 时钟输入模块 输入1KHZ，输出10、100、1KHZ nextfloor.bsf 电梯运行逻辑判断模块 判断是否运行、是否停层以及运行方向 updownkey.bsf 上下行按键输入模块 elevator.qpf 整机电路 floorruncount 楼层运行次数计数 5counter 5进制计数器 10counter 10进制计数器 4counter 4进制计数器 PCB制板电路原理图 整体电路原理 时钟电路 电机电路 按键输入 数码管显示 楼层指示及蜂鸣器 PCB布线图 样板测试 "},"Some Thing Fun/基于MFC的五子棋.html":{"url":"Some Thing Fun/基于MFC的五子棋.html","title":"基于MFC的五子棋","keywords":"","body":"Gobang MFC version \r 功能说明 效果展示 下棋部分 安装包部分 \r GitHub地址：https://github.com/ICEORY/GobangMFC 功能说明 漂亮的界面 实现功能菜单，左键下棋，右键选择菜单 实现双人对战或者人机对战 实现还不错的AI下棋算法(虽然还是很弱) 可以选择白子先手或者黑子先手 给执行文件加上图标 将代码打包成安装包 效果展示 下棋部分 安装包部分 "},"Some Thing Fun/cpp_table_v0.html":{"url":"Some Thing Fun/cpp_table_v0.html","title":"基于c++的表格设计v0","keywords":"","body":"Multi-Function Table GitHub地址：https://github.com/ICEORY/Learning-C-Plus-Plus/tree/master/multifunction-table 功能说明 创建一个表格，实现创建多个列，对每一列的数据类型以及列名进行定义，完成之后，可以对数据进行自动排序。 结果展示 "},"Some Thing Fun/cpp_table_v1.html":{"url":"Some Thing Fun/cpp_table_v1.html","title":"基于c++的表格设计v1","keywords":"","body":"Multi-Function Table Diamond GitHub地址：https://github.com/ICEORY/Learning-C-Plus-Plus/tree/master/multifunction-table-diamond 功能说明 输入表格名称，定义列数、列名称 定义列数据类型、输入列数据 新增功能：对数据进行自动计算 进行正序、倒序排序以及数据筛选 退出后可以选择继续创建表格或者选择所有创建的表格 继续退出之后显示退出界面 结果展示 "},"Some Thing Fun/cpp_table_v2.html":{"url":"Some Thing Fun/cpp_table_v2.html","title":"基于c++的表格设计v2","keywords":"","body":"Multi-Function Table Upgrade GitHub地址：https://github.com/ICEORY/Learning-C-Plus-Plus/tree/master/multifunction-table-upgrade 功能说明 输入表格名称以及表格的列数和列的名称 定义各个列的数据类型 完成之后可以实现对数据筛选、排序(包括正序和倒序) 退出 效果图 "},"Some Thing Fun/cpp_table_v3.html":{"url":"Some Thing Fun/cpp_table_v3.html","title":"基于c++的表格设计v3","keywords":"","body":"Multi-Function Table Upgrade Again GitHub地址：https://github.com/ICEORY/Learning-C-Plus-Plus/tree/master/multifunction-table-upgrade-again 功能说明 创建一个表格，输入表格名称，列数，列名称 输入各列的数据类型、数据内容 选择对数据进行正序或者倒序排序、筛选 退出表格 选择是否继续创建表格 重复以上步骤，或者可以选择显示所创建的各种表格或者退出 退出整个程序 结果展示 "},"Some Thing Fun/CPP扫雷游戏.html":{"url":"Some Thing Fun/CPP扫雷游戏.html","title":"CPP扫雷游戏","keywords":"","body":"Minesweeping GitHub地址：https://github.com/ICEORY/Minesweeping 功能说明 一个简单的扫雷游戏，主要实现游戏里面空格的索引： 点开一个空格之后相连的空格以及周围的数字都会显示出来 效果展示 "},"Some Thing Fun/MFC简易音乐播放器.html":{"url":"Some Thing Fun/MFC简易音乐播放器.html","title":"MFC简易音乐播放器","keywords":"","body":"Music Player GitHub地址：https://github.com/ICEORY/MusicPlayerMFC 功能说明 这就是一个简单的音乐播放器，练习MFC的时候做的 效果展示 "},"Some Thing Fun/基于VB的二次曲线.html":{"url":"Some Thing Fun/基于VB的二次曲线.html","title":"基于VB的二次曲线","keywords":"","body":"Quadratic Curves GitHub地址：https://github.com/ICEORY/LearningVB/tree/master/quadratic-curves 功能说明： 根据设定的参数绘制二次曲线 结果展示 a = 0.01, b = 0, c = -100 a = 0.1, b = 1, c = 1 a = 1, b = 1, c = 1 "},"Some Thing Fun/基于VB的五子棋.html":{"url":"Some Thing Fun/基于VB的五子棋.html","title":"基于VB的五子棋","keywords":"","body":"Gobang GitHub地址：https://github.com/ICEORY/LearningVB/tree/master/gobang 功能说明： 绘制基本的棋盘 双人对战 自动判定胜负 效果展示 "},"Some Thing Fun/基于VB的李萨如图形.html":{"url":"Some Thing Fun/基于VB的李萨如图形.html","title":"基于VB的李萨如图形","keywords":"","body":"Lissajous Figures GitHub地址：https://github.com/ICEORY/LearningVB/tree/master/lissajous-figures 功能说明： 绘制李萨如图形 结果展示 F1 = 10, P1 = 0.2pi, F2 = 10, P2 = 0pi F1 = 10, P1 = 0pi, F2 = 10, P2 = 0pi F1 = 200, P1 = 0.2pi, F2 = 10, P2 = 0pi "},"Some Thing Fun/机器人瓦力.html":{"url":"Some Thing Fun/机器人瓦力.html","title":"机器人瓦力","keywords":"","body":"机器人瓦力设计 \r 摘要 系统概述 系统软硬件实现 整体实现方案 硬件结构设计 软件设计 实物效果图 \r GitHub地址：https://github.com/ICEORY/WallyArduino 摘要 Arduino 作为一个开源的硬件平台，因为其使用门槛低以及使用方便的特点，受到了广大 DIY 爱好者的喜爱。许多开发者并不需要很好的硬件功底便可以利用该硬件平台进行电子硬件的设计，并实现酷炫的效果。本文主要介绍了一款基于Arduino mini 开发平台设计的智能机器人，并对该智能机器人的软件实现以及硬件设计做了详细的介绍。该款机器人具备了结构简单、趣味性强的特点，主要应用于机器人的教育领域。 本文根据电影《机器人总动员》中的主角瓦力的形象设计了一个能够自主运行的教育机器人，该机器人能够实现红外检测并根据红外检测的结果进行运动方式的切换。此外利用两个伺服电机设计了机器人的眼睛，使机器人能够实现眼部的运动，整体运行效果与机器人瓦力的效果相似。由于该机器人结构简单，运行效果非常有趣，很适合用于小学生的机器人课题教学。 系统概述 本课题主要基于 arduino mini 开发平台设计一个能够实现红外检测的履带式机器人。该机器人设计主要分为两大部分： 外观设计。该机器人采用机器人总动员中瓦力的外观，利用 KT 板或者安迪板切割进行构建，完成基本构架之后在表面上贴彩色的瓦力图像，机器人更加生动；该部分为本课题的重点，决定了整个作品的效果； 内部硬件设计。使用 arduino mini 进行硬件开发，传感器主要为红外传感器以及 LED，执行器为减速电机以及伺服电机；目标实现机器人能够进行自主的运动以及在检测到前方有障碍物时摇动眼睛。 系统软硬件实现 整体实现方案 本课题主要根据电影《机器人总动员》中瓦力的形象进行外观的设计：使用伺服电机以及 LED 构造机器人的眼睛，使眼睛能够上下摆动以及闪烁；使用减速电机以及履带轮构造机器人的足部，实现瓦力自由行走的效果。系统设计方案如图所示： 结构设计： 运动方式： 尺寸设计： 硬件结构设计 瓦力内部控制硬件主要包括如下几个部分： Arduino mini控制平台：作为中央控制器，负责对传感器数据的采集以及对执行机构的控制； 减速电机：作为机器人的足部动力机构，带动三角形的履带转动，实现机器人的左转、右转、前进、停止； LED：作为机器人的眼睛，能够判断前方是否有近距离障碍物； 伺服电机：控制机器人眼睛的运动，瓦力的一个重要特征是两个眼睛能够晃动，非常有趣； 软件设计 软件部分按照状态机的方式进行设计，状态切换开关为机器人眼部的红外检测模块，每次检测到变化都对机器人的运行状态进行改变，依次变化为直走、左转、右转、后退、停止，同时眼部进行摆动，眨眼动作。软件结构框图如下所示： 实物效果图 如下为搭建过程的效果以及最终的成果图： "},"Some Thing Fun/简易语音识别设备.html":{"url":"Some Thing Fun/简易语音识别设备.html","title":"简易语音识别设备","keywords":"","body":"基于MFC与MATLAB混合编程的语音识别系统设计 \r 摘要 研究内容 系统设计 单片机程序结构 MFC上位机系统结构 在线模式下的程序结构 测试模式下程序结构 测试模式下子窗口程序 MATLAB数据处理结构 界面设计 主界面 测试模式下子窗口 主窗口绘制动态曲线 数据处理结果 总结 单片机程序部分 MFC程序设计部分 MFC与MATLAB混合编程 基于MATLAB engine的混合编程 基于com组件的混合编程 \r GitHub地址：https://github.com/ICEORY/SpeechRecognition 摘要 本系统以MFC上位机系统作为程序控制的中部枢纽，通过VC自带的MSComm控件与单片机进行单向通信，将采集到的数据在人机界面上显示，同时将数据保存到MATLAB的mat数据文件中。在离线工作模式下自动调用MATLAB函数对数据进行滤波以及时频空间上的转换，并将处理结果以绘图形式呈现。 本文解决的问题主要是： 1.PC与单片机通信 2.MFC与调用matlab engine 进行混合编程 3.matlab简单语音识别算法 没有解决的问题： 语音识别的算法问题，在这里只是做一个简单的测试，并没对算法进行研究。 研究内容 本课题搭建的系统主要由三个部分构成： 基于STC89C52单片机的语音采集系统； 基于MFC Dialog的上位机系统； 基于MATLAB m文件的语音数据处理算法。 其中单片机与MFC之间利用MSComm控件进行单向通信，即单片机采集到的数据单方向发送到MFC的缓存区。而MFC与MATLAB程序之间通过matlab engine进行函数的调用，从而实现混合编程。最终实现采集环境声音并在离线模式下进行数据处理显示的功能。 系统设计 单片机程序结构 MFC上位机系统结构 在线模式下的程序结构 测试模式下程序结构 测试模式下子窗口程序 MATLAB数据处理结构 界面设计 主界面 主界面设计简洁明了，便于操作。测试者可以通过串口选择不同的串口编号，不同的USB转串口线在PC上显示的串口编号有所差异，所以要根据实际需要选择。采样数据以及采样频率在编辑框中实时显示，并能够在绘图区动态绘制采样数据形成的曲线。 测试模式下子窗口 主窗口绘制动态曲线 数据处理结果 在测试模式下采集完数据后，调用matlab函数对数据进行处理，结果如下所示，可以看出noise在频域上呈现均匀分布，而四个signal数据显示出语音在某段频率区间较强，而不同的目标声音之间也有差异。故可以利用这一特点区分不同的语音。 总结 单片机程序部分 在这一部分主要考虑的问题是单片机如何采集环境数据并发送出去。由于设备有限，我们采用STC89C52单片机，以及ADC0809搭建语音采集的部分。但是由于ADC0809本身没有内部时钟，需要外部输入一定频率的脉冲作为时钟信号。考虑到设计基于555芯片的时钟电路会增加电路设计的难度，也会提高电路的复杂程度，故直接采用单片机自带时钟作为触发。而另外的，由于进行串口通信时所采用的波特率由单片机的定时器1模式所决定，也就是需要消耗另外一个定时器用于确定通信波特率。但由于单片机本身仅有两个定时器，这便导致了采样频率不能由我们确定，而是由定时器1同时决定。故我们只能通过MFC上位机计算单片机的采样率。 另外存在的问题是，由于语音采集模块本身固有的缺陷，导则了我们不能采集到负电位的数据，而且采集模块电容的存在导致放电时间延长，也就是采集到的数据不能很好的反映实际的语音信号，会出现很长的电压衰减过程。在这个方面上来说会对数据的分析造成很大的影响。 MFC程序设计部分 MFC程序使用Microsoft控件工具箱中的MSComm控件进行数据获取。MSComm控件在参数设定正确的情况下可以通过PC的串口接收来自其他设备的数据信息。当外界发送数据到接收缓冲区时，MSComm会受到触发，进入OnMSComm的处理函数中进行数据处理，但是在处理完数据之后必须要把接收区清空以免对接下来的数据造成影响。 MSComm控件也可以发送数据到发送缓冲区并通过串口将数据发送到外围设备，但是在本课题中，考虑到与单片机通过双工模式进行通信会大大延长每一帧的采样时间，也就是会降低系统的对语音的分辨率，对本来已经性能不好的采样设备造成很大影响，故最终使用单向数据传送。但如此难免会产生数据流发送的错位，因此需要MFC对数据进行单方面的校验。只有符合数据规则的数据才被接收，否则舍弃。但如此还是会出现数据错位的情况。 在MFC使用MSComm控件主要出现的问题是，MSComm控件使用的环境设置问题。一开始不知道如何调用该控件，故经常出现类似于“不能识别的符号”等问题，也会出现没有lib文件，但是在进行相应的设置之后还是会出现“断言错误”等严重的错误。最后在多次尝试之后发现是与系统不兼容。Microsoft所带的MSComm控件是在32位的windows平台下才能够使用，暂时也没有找到64位平台下的MSComm控件，故只能把visualstudio的x64平台改回win32的平台，在这样的情况下才能够正常使用MSComm控件。然而这个与6位matlab程序冲突。这个问题在接下来的摸索中解决了。 关于MSComm控件的调用需要注意的是，在visual studio软件平台中并没有这个控件，故需要人工导入。导入的流程如下：打开visualstudio 2010b->Tools->Choose Toolbox Items->COM Components->选择 Microsoft Communications Control, version 6.0即可以将MSComm控件加入到控件工具箱中使用。然后利用Class Warze创建一个MSComm类。理论上则可以使用该控件进行串口通信了，但是实际上创建的MSComm类中的cpp文件是空的，很多函数都缺失了，故仍需要从网上下载相关的cpp文件。 MFC与MATLAB混合编程 基于MATLAB engine的混合编程 使用matlab混合编程时，由于是使用engine调用函数在matlab java虚拟机的环境下编程，理论上可以实现matlab所有的功能。但由于没有完全脱离matlab平台，故在运行方面仍是比较麻烦，需要的内存也比较大。 Visual studio 在使用其他软件的函数时通常都要进行相关的环境设置。在使用MATLAB engine进行混合编程时，需要将matlab需要的头文件以及库文件等添加到visual studio的搜寻路径中。通常设置如下：在Class View 的界面下对建立的工程点击右键，选择properties，出现如下界面： 在VC++ Directies中添加library以及include的搜寻路径，在linker中添加所需要的lib文件名称。这样便可解决调用matlab engine过程中出现的大部分“不能识别符号”的错误了。 但仍需要注意的是，在安装的matlab\\extern\\include或者matlab\\extern\\lib文件夹中并没有所有的需要的文件，大部分的lib,dll文件默认保存在matlab\\bin的目录下，故需要把文件拷贝到include目录下，才能正常使用。 在解决完这一切之后，会出现的问题是前文提到的版本兼容问题。由于一开始安装的matlab是64位的，故只能是在visualstudio的x64编译平台下使用，在win32下使用常会出现严重的“断言错误”，导致程序不能正常运行。故在查阅了大量网络资料发现这个问题之后，把原来win32的平台改成x64的平台，从而可以正常使用64位的matlab。但在接下来使用MSComm的时候不得已又因为兼容问题把x64改回win32，并且将matlab重新安装成32位。故实现了同时使用MSComm控件和matlab engine. 此外面临的问题是，visual studio的环境中会出现不能正常使用用户自己编的函数的问题。在多次尝试之后发现，需要将用户自编的函数添加到matlab的搜寻路径中才能够正常使用。需要注意的问题是，使用matlab engine并不会运行matlab软件平台，而是运行matlab的java虚拟机以节省运行所需要的内存。也就是需要在虚拟机中将函数所在文件路径添加到搜寻路径中，而不是在软件平台下添加。在处理完这个问题之后基本是可以进行混合编程了。 基于com组件的混合编程 Matlab平台带有的编译器可以编译产生com组件，也就是可以将matlab函数所需要用的函数都打包成动态链接库的形式，并将用户自编的函数编译成cpp文件。在理论在，com组件可以在任何平台下使用。 最初打算使用com组件进行混合编程，但是由于版本问题，matlab所带的部分dll文件仍为64位系统使用，不能在32位的情况下使用，导致了com组件的产生失败。而通过命令语句的形式调用matlab MCC编译器产生cpp文件最后都因为缺少必要的dll文件而不能顺利使用。在这个问题没有解决的情况下最终决定使用matlabengine进行混合编程。 "},"Thinking/20171007.html":{"url":"Thinking/20171007.html","title":"2017.10.07","keywords":"","body":"2017年10月7日 启 经过一段时间的思考，也找了挺多的资料，最终没有发现自己想要的网页笔记。主要应该是个人比较懒一点，对这些文件的管理不是很到位，也就希望借助现有的一些工具来实现。不过因为最后没有合适的，所以还是使用gitbook来构建自己的一个空间。 离毕业还剩下一年不到的时间，也趁着这最后的一点时间留一些痕迹。即使是养成一个写博客的习惯也是挺好的，希望能够坚持下去吧。之后回过头看自己写的东西，感觉还是蛮有成就感的。不过考虑到这个空间的开放性，还有自己的一些初衷。博客尽量还是写的好看一点吧，至于语言的话，中英混合，根据需要采用对应的语言，也就不在意这些细节了。 也就不立什么旗帜了，个人随心就好。 "}}